import asyncio
import os
import logging
from typing import Any, Optional

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
from app.models.models import CrawlRequest, CrawlData, MarkdownRequest, MarkdownData, MarkdownFormat, ScreenshotRequest, ScreenshotData

logger = logging.getLogger(__name__)


class CrawlerException(Exception):
    """çˆ¬è™«å¼‚å¸¸ç±»"""

    def __init__(self, message: str, error_type: str = "unknown"):
        self.message = message
        self.error_type = error_type
        super().__init__(message)


class CrawlerService:
    """çˆ¬è™«æœåŠ¡ç±»"""

    @staticmethod
    def _is_debug_mode() -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè°ƒè¯•æ¨¡å¼"""
        return os.environ.get('CRAWLER_DEBUG_MODE', 'false').lower() == 'true'

    @staticmethod
    def _get_extension_path() -> Optional[str]:
        """è·å–æ‰©å±•è·¯å¾„"""
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®
        env_path = os.environ.get('CHROME_EXTENSION_PATH')
        if env_path and os.path.exists(env_path):
            return env_path

        # é»˜è®¤æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ chrome-extension æ–‡ä»¶å¤¹
        project_extension_path = Path(
            "./chrome-extension/bypass-paywalls-chrome-clean")
        if project_extension_path.exists():
            return str(project_extension_path.resolve())

        # å¤‡é€‰è·¯å¾„ 1: download æ–‡ä»¶å¤¹
        download_path = Path("./download/bypass-paywalls-chrome-clean-master")
        if download_path.exists():
            return str(download_path.resolve())

        # å¤‡é€‰è·¯å¾„ 2: ç”¨æˆ·ä¸‹è½½ç›®å½•
        home_download_path = Path.home() / "Downloads" / \
            "bypass-paywalls-chrome-clean-master"
        if home_download_path.exists():
            return str(home_download_path.resolve())

        return None

    @staticmethod
    def _create_browser_config(js_enabled: bool = True) -> BrowserConfig:
        """åˆ›å»ºæµè§ˆå™¨é…ç½®"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰©å±•éœ€è¦åŠ è½½
        extension_path = CrawlerService._get_extension_path()

        # æ£€æŸ¥è°ƒè¯•æ¨¡å¼
        debug_mode = CrawlerService._is_debug_mode()

        # å¦‚æœæœ‰æ‰©å±•æˆ–è°ƒè¯•æ¨¡å¼ï¼Œå¼ºåˆ¶ä½¿ç”¨éæ— å¤´æ¨¡å¼
        headless = True
        if extension_path:
            headless = False
            logger.info(f"ğŸ”Œ æ£€æµ‹åˆ°æ‰©å±•ï¼Œå°†ä½¿ç”¨éæ— å¤´æ¨¡å¼: {extension_path}")
        elif debug_mode:
            headless = False
            logger.info(f"ğŸ› è°ƒè¯•æ¨¡å¼å¯ç”¨ï¼Œå°†ä½¿ç”¨éæ— å¤´æ¨¡å¼")

        browser_config = BrowserConfig(
            headless=headless,
            java_script_enabled=js_enabled,
            viewport={"width": 1280, "height": 800},
            verbose=True  # è°ƒè¯•æ—¶å¯ç”¨è¯¦ç»†æ—¥å¿—
        )

        # æ·»åŠ æ‰©å±•æ”¯æŒ
        if extension_path:
            extension_args = [
                f"--load-extension={extension_path}",
                f"--disable-extensions-except={extension_path}",
                "--disable-extensions-except-devtools"
            ]

            if not hasattr(browser_config, 'extra_args') or browser_config.extra_args is None:
                browser_config.extra_args = []

            browser_config.extra_args.extend(extension_args)
            logger.info(f"ğŸ”Œ å·²æ·»åŠ æ‰©å±•å‚æ•°: {extension_args}")

        # è°ƒè¯•æ¨¡å¼ä¸‹æ·»åŠ é¢å¤–å‚æ•°
        if debug_mode:
            debug_args = [
                "--disable-web-security",  # ç¦ç”¨webå®‰å…¨é™åˆ¶
                "--disable-features=VizDisplayCompositor",  # æé«˜å…¼å®¹æ€§
                "--allow-running-insecure-content",  # å…è®¸ä¸å®‰å…¨å†…å®¹
            ]

            if not hasattr(browser_config, 'extra_args') or browser_config.extra_args is None:
                browser_config.extra_args = []

            browser_config.extra_args.extend(debug_args)
            logger.info(f"ğŸ› å·²æ·»åŠ è°ƒè¯•å‚æ•°: {debug_args}")

        return browser_config

    @staticmethod
    def _create_crawler_config(request: CrawlRequest) -> CrawlerRunConfig:
        """åˆ›å»ºçˆ¬è™«é…ç½®"""
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
            page_timeout=60000,  # 60ç§’
            wait_for_images=request.include_images,
        )

        if request.css_selector:
            config.css_selector = request.css_selector

        return config

    @staticmethod
    def _create_markdown_crawler_config(request: MarkdownRequest) -> CrawlerRunConfig:
        """åˆ›å»ºMarkdownä¸“ç”¨çˆ¬è™«é…ç½®"""
        # åˆ›å»ºMarkdownç”Ÿæˆå™¨é…ç½®
        md_options = {}
        if request.ignore_links:
            md_options["ignore_links"] = True
        if not request.escape_html:
            md_options["escape_html"] = False
        if request.body_width:
            md_options["body_width"] = request.body_width

        # æ ¹æ®æ ¼å¼ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨å†…å®¹è¿‡æ»¤
        if request.format in [MarkdownFormat.FIT, MarkdownFormat.BOTH]:
            # ä½¿ç”¨å†…å®¹è¿‡æ»¤å™¨ç”Ÿæˆæ›´é€‚åˆAIçš„markdown
            content_filter = PruningContentFilter(
                threshold=0.4, threshold_type="fixed")
            md_generator = DefaultMarkdownGenerator(
                content_filter=content_filter,
                options=md_options
            )
        else:
            # åŸå§‹markdownï¼Œä¸ä½¿ç”¨è¿‡æ»¤å™¨
            md_generator = DefaultMarkdownGenerator(options=md_options)

        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
            page_timeout=60000,  # 60ç§’
            markdown_generator=md_generator,
        )

        if request.css_selector:
            config.css_selector = request.css_selector

        return config

    async def crawl_url(self, request: CrawlRequest) -> CrawlData:
        """
        çˆ¬å–å•ä¸ªURL - è¿”å›çº¯ä¸šåŠ¡æ•°æ®æˆ–æŠ›å‡ºå¼‚å¸¸

        Args:
            request: çˆ¬å–è¯·æ±‚å¯¹è±¡

        Returns:
            CrawlData: çˆ¬å–çš„ä¸šåŠ¡æ•°æ®

        Raises:
            CrawlerException: çˆ¬å–å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            browser_config = self._create_browser_config(request.js_enabled)
            crawler_config = self._create_crawler_config(request)

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=crawler_config)

                if not result.success:
                    raise CrawlerException(
                        message=getattr(result, 'error_message', 'çˆ¬å–å¤±è´¥'),
                        error_type="crawl_failed"
                    )

                # è¿”å›çº¯ä¸šåŠ¡æ•°æ®
                return CrawlData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    markdown=result.markdown,
                    media=result.media if hasattr(result, 'media') else None,
                    links=result.links if hasattr(result, 'links') else None
                )

        except asyncio.TimeoutError:
            logger.error(f"çˆ¬å–è¶…æ—¶: {request.url}")
            raise CrawlerException(
                message="çˆ¬å–è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                error_type="timeout"
            )
        except CrawlerException:
            # é‡æ–°æŠ›å‡ºå·²çŸ¥å¼‚å¸¸
            raise
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    async def crawl_markdown(self, request: MarkdownRequest) -> MarkdownData:
        """
        ä¸“é—¨è·å–é¡µé¢çš„Markdownå†…å®¹ - è¿”å›çº¯ä¸šåŠ¡æ•°æ®æˆ–æŠ›å‡ºå¼‚å¸¸

        Args:
            request: Markdownè¯·æ±‚å¯¹è±¡

        Returns:
            MarkdownData: Markdownä¸šåŠ¡æ•°æ®

        Raises:
            CrawlerException: çˆ¬å–å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            browser_config = self._create_browser_config(request.js_enabled)
            crawler_config = self._create_markdown_crawler_config(request)

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=crawler_config)

                if not result.success:
                    raise CrawlerException(
                        message=getattr(
                            result, 'error_message', 'Markdownè·å–å¤±è´¥'),
                        error_type="crawl_failed"
                    )

                # è§£æç»“æœ
                title = None
                if hasattr(result, 'metadata') and result.metadata:
                    title = result.metadata.get('title')

                raw_markdown = None
                fit_markdown = None

                if hasattr(result, 'markdown'):
                    if request.format in [MarkdownFormat.RAW, MarkdownFormat.BOTH]:
                        raw_markdown = self._extract_raw_markdown(
                            result.markdown)

                    if request.format in [MarkdownFormat.FIT, MarkdownFormat.BOTH]:
                        fit_markdown = self._extract_fit_markdown(
                            result.markdown, raw_markdown)

                # è®¡ç®—å­—æ•°
                word_count = None
                if raw_markdown:
                    word_count = len(raw_markdown.split())
                elif fit_markdown:
                    word_count = len(fit_markdown.split())

                return MarkdownData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    raw_markdown=raw_markdown,
                    fit_markdown=fit_markdown,
                    title=title,
                    word_count=word_count
                )

        except asyncio.TimeoutError:
            logger.error(f"Markdownçˆ¬å–è¶…æ—¶: {request.url}")
            raise CrawlerException(
                message="Markdownè·å–è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                error_type="timeout"
            )
        except CrawlerException:
            raise
        except Exception as e:
            logger.error(f"Markdownçˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"Markdownè·å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    @staticmethod
    def _extract_raw_markdown(markdown_result) -> str:
        """æå–åŸå§‹markdownå†…å®¹"""
        if hasattr(markdown_result, 'raw_markdown'):
            return markdown_result.raw_markdown
        else:
            # å‘åå…¼å®¹ï¼Œå¦‚æœæ²¡æœ‰raw_markdownå±æ€§ï¼Œä½¿ç”¨markdownæœ¬èº«
            return markdown_result if isinstance(markdown_result, str) else str(markdown_result)

    @staticmethod
    def _extract_fit_markdown(markdown_result, raw_markdown: str = None) -> str:
        """æå–ç»è¿‡è¿‡æ»¤çš„markdownå†…å®¹"""
        if hasattr(markdown_result, 'fit_markdown'):
            return markdown_result.fit_markdown
        else:
            # å¦‚æœæ²¡æœ‰fit_markdownï¼Œä½¿ç”¨raw_markdownä½œä¸ºå¤‡é€‰
            return raw_markdown

    @staticmethod
    def _create_screenshot_browser_config(request: ScreenshotRequest) -> BrowserConfig:
        """åˆ›å»ºæˆªå›¾ä¸“ç”¨æµè§ˆå™¨é…ç½®"""
        return BrowserConfig(
            headless=True,
            java_script_enabled=request.js_enabled,
            viewport={"width": request.viewport_width or 1280,
                      "height": request.viewport_height or 720},
            verbose=False
        )

    @staticmethod
    def _create_screenshot_crawler_config(request: ScreenshotRequest) -> CrawlerRunConfig:
        """åˆ›å»ºæˆªå›¾ä¸“ç”¨çˆ¬è™«é…ç½®"""
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
            page_timeout=60000,  # 60ç§’
            screenshot=True,
        )

        if request.css_selector:
            config.css_selector = request.css_selector

        if request.wait_for:
            config.wait_until = request.wait_for

        return config

    async def take_screenshot(self, request: ScreenshotRequest) -> ScreenshotData:
        """
        æˆªå–é¡µé¢æˆªå›¾ - è¿”å›çº¯ä¸šåŠ¡æ•°æ®æˆ–æŠ›å‡ºå¼‚å¸¸

        Args:
            request: æˆªå›¾è¯·æ±‚å¯¹è±¡

        Returns:
            ScreenshotData: æˆªå›¾ä¸šåŠ¡æ•°æ®

        Raises:
            CrawlerException: æˆªå›¾å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            browser_config = self._create_screenshot_browser_config(request)
            crawler_config = self._create_screenshot_crawler_config(request)

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=crawler_config)

                if not result.success:
                    raise CrawlerException(
                        message=getattr(result, 'error_message', 'æˆªå›¾å¤±è´¥'),
                        error_type="screenshot_failed"
                    )

                if not result.screenshot:
                    raise CrawlerException(
                        message="æˆªå›¾æ•°æ®ä¸ºç©º",
                        error_type="screenshot_empty"
                    )

                # è¿”å›çº¯ä¸šåŠ¡æ•°æ®
                return ScreenshotData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    screenshot_base64=result.screenshot,
                    error_message=None
                )

        except asyncio.TimeoutError:
            logger.error(f"æˆªå›¾è¶…æ—¶: {request.url}")
            raise CrawlerException(
                message="æˆªå›¾è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                error_type="timeout"
            )
        except CrawlerException:
            # é‡æ–°æŠ›å‡ºå·²çŸ¥å¼‚å¸¸
            raise
        except Exception as e:
            logger.error(f"æˆªå›¾å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"æˆªå›¾è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )


# åˆ›å»ºæœåŠ¡å®ä¾‹
crawler_service = CrawlerService()

import asyncio
import os
import logging
from typing import Any, Optional

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
from app.models.models import CrawlRequest, CrawlData, MarkdownRequest, MarkdownData, MarkdownFormat, ScreenshotRequest, ScreenshotData

logger = logging.getLogger(__name__)


class CrawlerException(Exception):
    """çˆ¬è™«å¼‚å¸¸ç±»"""

    def __init__(self, message: str, error_type: str = "unknown"):
        self.message = message
        self.error_type = error_type
        super().__init__(message)


class CrawlerService:
    """çˆ¬è™«æœåŠ¡ç±»"""

    @staticmethod
    def _is_debug_mode() -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè°ƒè¯•æ¨¡å¼"""
        return os.environ.get('CRAWLER_DEBUG_MODE', 'false').lower() == 'true'

    @staticmethod
    def _get_extension_path() -> Optional[str]:
        """è·å–æ‰©å±•è·¯å¾„"""
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®
        env_path = os.environ.get('CHROME_EXTENSION_PATH')
        if env_path and os.path.exists(env_path):
            return env_path

        # é»˜è®¤æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ chrome-extension æ–‡ä»¶å¤¹
        from pathlib import Path
        project_extension_path = Path(
            "./chrome-extension/bypass-paywalls-chrome-clean")
        if project_extension_path.exists():
            return str(project_extension_path.resolve())

        # å¤‡é€‰è·¯å¾„ 1: download æ–‡ä»¶å¤¹
        download_path = Path("./download/bypass-paywalls-chrome-clean-master")
        if download_path.exists():
            return str(download_path.resolve())

        # å¤‡é€‰è·¯å¾„ 2: ç”¨æˆ·ä¸‹è½½ç›®å½•
        home_download_path = Path.home() / "Downloads" / \
            "bypass-paywalls-chrome-clean-master"
        if home_download_path.exists():
            return str(home_download_path.resolve())

        return None

    # @staticmethod
    # def _create_browser_config(js_enabled: bool = True) -> BrowserConfig:
    #     """åˆ›å»ºæµè§ˆå™¨é…ç½® - å•å®ä¾‹ä¿®å¤ç‰ˆæœ¬"""
    #     # æ£€æŸ¥æ˜¯å¦æœ‰æ‰©å±•éœ€è¦åŠ è½½
    #     extension_path = CrawlerService._get_extension_path()

    #     # æ£€æŸ¥è°ƒè¯•æ¨¡å¼
    #     debug_mode = CrawlerService._is_debug_mode()

    #     # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœæœ‰æ‰©å±•ï¼Œå¼ºåˆ¶ä½¿ç”¨éæ— å¤´æ¨¡å¼ä»¥ç¡®ä¿æ’ä»¶æ­£å¸¸å·¥ä½œ
    #     headless = True
    #     user_data_dir = None

    #     if extension_path:
    #         headless = False  # æ’ä»¶åœ¨æ— å¤´æ¨¡å¼ä¸‹å¯èƒ½ä¸å·¥ä½œ
    #         # ğŸ†• ä½¿ç”¨æŒä¹…åŒ–ç”¨æˆ·æ•°æ®ç›®å½•ç¡®ä¿æ’ä»¶é…ç½®ä¿æŒ
    #         user_data_dir = "./extension_browser_profile"
    #         logger.info(f"ğŸ”Œ æ£€æµ‹åˆ°æ‰©å±•ï¼Œä½¿ç”¨å•å®ä¾‹é…ç½®: {extension_path}")
    #     elif debug_mode:
    #         headless = False
    #         user_data_dir = "./debug_browser_profile"
    #         logger.info(f"ğŸ› è°ƒè¯•æ¨¡å¼å¯ç”¨ï¼Œä½¿ç”¨å•å®ä¾‹é…ç½®")

    #     browser_config = BrowserConfig(
    #         headless=headless,
    #         java_script_enabled=js_enabled,
    #         viewport={"width": 1280, "height": 800},
    #         verbose=True,
    #         # ğŸ†• å…³é”®é…ç½®ï¼šç¡®ä¿å•ä¸€æµè§ˆå™¨å®ä¾‹
    #         user_data_dir=user_data_dir,
    #         use_persistent_context=True if user_data_dir else False,
    #     )

    #     # ğŸ”§ ä¿®å¤æ‰©å±•é…ç½®
    #     if extension_path:
    #         extension_args = [
    #             f"--load-extension={extension_path}",
    #             f"--disable-extensions-except={extension_path}",
    #             "--disable-extensions-except-devtools",
    #             "--enable-extensions",  # ç¡®ä¿æ‰©å±•å¯ç”¨
    #             # ğŸ†• å•å®ä¾‹ç›¸å…³å‚æ•°
    #             "--no-first-run",
    #             "--no-default-browser-check",
    #             "--disable-default-apps",
    #             # ğŸ†• ä»˜è´¹å¢™ç»•è¿‡ç›¸å…³å‚æ•°
    #             "--disable-web-security",  # å¯¹æŸäº›ä»˜è´¹å¢™ç»•è¿‡æœ‰å¸®åŠ©
    #             "--disable-features=VizDisplayCompositor",  # æé«˜å…¼å®¹æ€§
    #             "--allow-running-insecure-content",  # å…è®¸ä¸å®‰å…¨å†…å®¹
    #             # ğŸ†• ç¡®ä¿æ‰©å±•åœ¨æ­£ç¡®çš„è¿›ç¨‹ä¸­è¿è¡Œ
    #             "--disable-extensions-file-access-check",
    #             "--enable-extension-activity-logging",
    #         ]

    #         if not hasattr(browser_config, 'extra_args') or browser_config.extra_args is None:
    #             browser_config.extra_args = []

    #         browser_config.extra_args.extend(extension_args)
    #         logger.info(f"ğŸ”Œ å·²é…ç½®æ‰©å±•å‚æ•°ï¼Œæ€»æ•°: {len(extension_args)}")

    #     # è°ƒè¯•æ¨¡å¼ä¸‹æ·»åŠ é¢å¤–å‚æ•°
    #     if debug_mode:
    #         debug_args = [
    #             "--no-first-run",
    #             "--no-default-browser-check",
    #         ]

    #         if not hasattr(browser_config, 'extra_args') or browser_config.extra_args is None:
    #             browser_config.extra_args = []

    #         browser_config.extra_args.extend(debug_args)
    #         logger.info(f"ğŸ› å·²æ·»åŠ è°ƒè¯•å‚æ•°: {debug_args}")

    #     return browser_config

    @staticmethod
    def _create_browser_config(js_enabled: bool = True, force_headless: Optional[bool] = None) -> BrowserConfig:
        """åˆ›å»ºæµè§ˆå™¨é…ç½® - æ”¯æŒå¼ºåˆ¶æ— å¤´æ¨¡å¼"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰©å±•éœ€è¦åŠ è½½
        extension_path = CrawlerService._get_extension_path()

        # æ£€æŸ¥è°ƒè¯•æ¨¡å¼
        debug_mode = CrawlerService._is_debug_mode()

        # ğŸ”§ æ™ºèƒ½headlessæ¨¡å¼å†³ç­–
        if force_headless is not None:
            # å¼ºåˆ¶æŒ‡å®šæ¨¡å¼ï¼ˆç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
            headless = force_headless
            user_data_dir = "./extension_browser_profile" if extension_path else None
            logger.info(f"ğŸ¯ å¼ºåˆ¶{'æ— å¤´' if headless else 'å¯è§'}æ¨¡å¼")
        elif extension_path:
            # æœ‰æ‰©å±•æ—¶ï¼Œé»˜è®¤éæ— å¤´ï¼ˆä½†å¯ä»¥è¢«force_headlessè¦†ç›–ï¼‰
            headless = False
            user_data_dir = "./extension_browser_profile"
            logger.info(f"ğŸ”Œ æ£€æµ‹åˆ°æ‰©å±•ï¼Œä½¿ç”¨å¯è§æ¨¡å¼: {extension_path}")
        elif debug_mode:
            headless = False
            user_data_dir = "./debug_browser_profile"
            logger.info(f"ğŸ› è°ƒè¯•æ¨¡å¼å¯ç”¨ï¼Œä½¿ç”¨å¯è§æ¨¡å¼")
        else:
            headless = True
            user_data_dir = None

        browser_config = BrowserConfig(
            headless=headless,
            java_script_enabled=js_enabled,
            viewport={"width": 1280, "height": 800},
            verbose=True,
            # ä½¿ç”¨æŒä¹…åŒ–é…ç½®ï¼ˆå¦‚æœæœ‰æ‰©å±•ï¼‰
            user_data_dir=user_data_dir,
            use_persistent_context=True if user_data_dir else False,
        )

        # ğŸ”§ æ‰©å±•é…ç½®
        if extension_path:
            extension_args = [
                f"--load-extension={extension_path}",
                f"--disable-extensions-except={extension_path}",
                "--disable-extensions-except-devtools",
                "--enable-extensions",
                "--no-first-run",
                "--no-default-browser-check",
                "--disable-default-apps",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
                "--allow-running-insecure-content",
                "--disable-extensions-file-access-check",
            ]

            if not hasattr(browser_config, 'extra_args') or browser_config.extra_args is None:
                browser_config.extra_args = []

            browser_config.extra_args.extend(extension_args)
            logger.info(f"ğŸ”Œ å·²é…ç½®æ‰©å±•å‚æ•°ï¼Œæ— å¤´æ¨¡å¼: {headless}")

        return browser_config

    @staticmethod
    def _create_crawler_config(request: CrawlRequest) -> CrawlerRunConfig:
        """åˆ›å»ºçˆ¬è™«é…ç½®"""
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
            page_timeout=60000,  # 60ç§’
            wait_for_images=request.include_images,
        )

        if request.css_selector:
            config.css_selector = request.css_selector

        return config

    # @staticmethod
    # def _create_markdown_crawler_config(request: MarkdownRequest) -> CrawlerRunConfig:
    #     """åˆ›å»ºMarkdownä¸“ç”¨çˆ¬è™«é…ç½® - ä¿®å¤ç‰ˆæœ¬"""
    #     # åˆ›å»ºMarkdownç”Ÿæˆå™¨é…ç½®
    #     md_options = {}
    #     if request.ignore_links:
    #         md_options["ignore_links"] = True
    #     if not request.escape_html:
    #         md_options["escape_html"] = False
    #     if request.body_width:
    #         md_options["body_width"] = request.body_width

    #     # æ ¹æ®æ ¼å¼ç±»å‹é€‰æ‹©æ˜¯å¦ä½¿ç”¨å†…å®¹è¿‡æ»¤
    #     if request.format in [MarkdownFormat.FIT, MarkdownFormat.BOTH]:
    #         # ä½¿ç”¨å†…å®¹è¿‡æ»¤å™¨ç”Ÿæˆæ›´é€‚åˆAIçš„markdown
    #         content_filter = PruningContentFilter(
    #             threshold=0.4, threshold_type="fixed")
    #         md_generator = DefaultMarkdownGenerator(
    #             content_filter=content_filter,
    #             options=md_options
    #         )
    #     else:
    #         # åŸå§‹markdownï¼Œä¸ä½¿ç”¨è¿‡æ»¤å™¨
    #         md_generator = DefaultMarkdownGenerator(options=md_options)

    #     config = CrawlerRunConfig(
    #         cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
    #         page_timeout=90000,  # ğŸ”§ å¢åŠ è¶…æ—¶æ—¶é—´ç»™æ’ä»¶æ›´å¤šæ—¶é—´å·¥ä½œ
    #         markdown_generator=md_generator,
    #         wait_for_images=True,  # ğŸ†• ç­‰å¾…å›¾ç‰‡åŠ è½½å®Œæˆ
    #     )

    #     if request.css_selector:
    #         config.css_selector = request.css_selector

    #     return config

    @staticmethod
    def _create_markdown_crawler_config(request: MarkdownRequest) -> CrawlerRunConfig:
        """åˆ›å»ºMarkdownä¸“ç”¨çˆ¬è™«é…ç½® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œè·å¾—æ›´å¹²å‡€çš„å†…å®¹"""

        # ğŸ†• åŸºäº crawl4ai å®˜æ–¹æ–‡æ¡£çš„ä¼˜åŒ–å‚æ•°
        md_options = {
            # é“¾æ¥å¤„ç†ä¼˜åŒ–
            "ignore_links": True,  # å®Œå…¨å¿½ç•¥æ‰€æœ‰é“¾æ¥ï¼Œè·å¾—çº¯æ–‡æœ¬
            "skip_internal_links": True,  # è·³è¿‡å†…éƒ¨é”šç‚¹é“¾æ¥
            "escape_html": False,  # ä¸è½¬ä¹‰HTMLï¼Œä¿æŒå†…å®¹æµç•…

            # æ ¼å¼ä¼˜åŒ–
            "body_width": 0,  # ä¸é™åˆ¶è¡Œå®½ï¼Œä¿æŒè‡ªç„¶æ¢è¡Œ
            "mark_code": True,  # æ ‡è®°ä»£ç å—
            "handle_code_in_pre": True,  # å¤„ç† <pre> æ ‡ç­¾ä¸­çš„ä»£ç 

            # ğŸ†• é«˜çº§é€‰é¡¹
            "include_sup_sub": False,  # ç®€åŒ–ä¸Šä¸‹æ ‡å¤„ç†
            "unicode_snob": True,  # ä¼˜åŒ–Unicodeå¤„ç†
            "default_image_alt": "",  # å›¾ç‰‡é»˜è®¤altæ–‡æœ¬
        }

        # æ ¹æ®ç”¨æˆ·è¯·æ±‚è°ƒæ•´ç‰¹å®šå‚æ•°
        if request.ignore_links:
            md_options["ignore_links"] = True
        if not request.escape_html:
            md_options["escape_html"] = False
        if request.body_width:
            md_options["body_width"] = request.body_width

        # ğŸ†• é€‰æ‹©æ›´å¼ºåŠ›çš„å†…å®¹è¿‡æ»¤å™¨
        if request.format in [MarkdownFormat.FIT, MarkdownFormat.BOTH]:
            # ä½¿ç”¨æ›´æ¿€è¿›çš„pruningè¿‡æ»¤å™¨å»é™¤å™ªéŸ³
            content_filter = PruningContentFilter(
                threshold=0.3,  # æ›´ä½çš„é˜ˆå€¼ï¼Œæ›´æ¿€è¿›åœ°ç§»é™¤å™ªéŸ³
                threshold_type="dynamic",  # åŠ¨æ€é˜ˆå€¼ï¼Œæ›´æ™ºèƒ½
                min_word_threshold=10,  # è‡³å°‘10ä¸ªè¯æ‰ä¿ç•™
                # ğŸ†• æ–°å¢å‚æ•°
                excluded_tags=['nav', 'header', 'footer',
                               'aside', 'menu'],  # æ’é™¤å¯¼èˆªç›¸å…³æ ‡ç­¾
                exclude_external_links=True,  # æ’é™¤å¤–éƒ¨é“¾æ¥
                word_count_threshold=15,  # æ®µè½è‡³å°‘15ä¸ªè¯
            )

            md_generator = DefaultMarkdownGenerator(
                content_filter=content_filter,
                options=md_options
            )
        else:
            # åŸå§‹markdownï¼Œä½†ä»ç„¶åº”ç”¨åŸºç¡€æ¸…ç†
            md_generator = DefaultMarkdownGenerator(options=md_options)

        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
            page_timeout=90000,
            markdown_generator=md_generator,
            wait_for_images=True,

            # ğŸ†• é¢å¤–çš„é¡µé¢çº§åˆ«ä¼˜åŒ–
            excluded_tags=['nav', 'header', 'footer',
                           'aside', 'script', 'style', 'noscript'],
            remove_overlay_elements=True,  # ç§»é™¤è¦†ç›–å…ƒç´ 
        )

        if request.css_selector:
            config.css_selector = request.css_selector

        return config

    async def crawl_url(self, request: CrawlRequest) -> CrawlData:
        """
        çˆ¬å–å•ä¸ªURL - è¿”å›çº¯ä¸šåŠ¡æ•°æ®æˆ–æŠ›å‡ºå¼‚å¸¸

        Args:
            request: çˆ¬å–è¯·æ±‚å¯¹è±¡

        Returns:
            CrawlData: çˆ¬å–çš„ä¸šåŠ¡æ•°æ®

        Raises:
            CrawlerException: çˆ¬å–å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            browser_config = self._create_browser_config(request.js_enabled)
            crawler_config = self._create_crawler_config(request)

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=crawler_config)

                if not result.success:
                    raise CrawlerException(
                        message=getattr(result, 'error_message', 'çˆ¬å–å¤±è´¥'),
                        error_type="crawl_failed"
                    )

                # è¿”å›çº¯ä¸šåŠ¡æ•°æ®
                return CrawlData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    markdown=result.markdown,
                    media=result.media if hasattr(result, 'media') else None,
                    links=result.links if hasattr(result, 'links') else None
                )

        except asyncio.TimeoutError:
            logger.error(f"çˆ¬å–è¶…æ—¶: {request.url}")
            raise CrawlerException(
                message="çˆ¬å–è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                error_type="timeout"
            )
        except CrawlerException:
            # é‡æ–°æŠ›å‡ºå·²çŸ¥å¼‚å¸¸
            raise
        except Exception as e:
            logger.error(f"çˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    # async def crawl_markdown(self, request: MarkdownRequest) -> MarkdownData:
    #     """
    #     ä¸“é—¨è·å–é¡µé¢çš„Markdownå†…å®¹ - å•å®ä¾‹ä¿®å¤ç‰ˆæœ¬
    #     """
    #     try:
    #         browser_config = self._create_browser_config(request.js_enabled)
    #         crawler_config = self._create_markdown_crawler_config(request)

    #         # ğŸ”§ å¦‚æœæ£€æµ‹åˆ°æ‰©å±•ï¼Œå¢åŠ é¢å¤–çš„ç­‰å¾…å’Œé¢„çƒ­
    #         extension_path = self._get_extension_path()
    #         if extension_path:
    #             logger.info(f"ğŸ”Œ ä½¿ç”¨æ‰©å±•æ¨¡å¼æŠ“å–ï¼Œç¡®ä¿å•å®ä¾‹è¿è¡Œ")

    #         async with AsyncWebCrawler(config=browser_config) as crawler:
    #             # ğŸ†• æ‰©å±•é¢„çƒ­æœºåˆ¶ - æ›´åŠ ç¨³å¥
    #             if extension_path:
    #                 logger.info("ğŸ”¥ æ‰©å±•é¢„çƒ­ä¸­...")
    #                 try:
    #                     # 1. é¦–å…ˆè®¿é—®æ‰©å±•ç®¡ç†é¡µé¢ç¡®ä¿æ‰©å±•åŠ è½½
    #                     warmup_config = CrawlerRunConfig(
    #                         cache_mode=CacheMode.BYPASS,
    #                         page_timeout=15000
    #                     )
    #                     await crawler.arun(url="chrome://extensions/", config=warmup_config)
    #                     await asyncio.sleep(3)

    #                     # 2. å†è®¿é—®ä¸€ä¸ªç®€å•é¡µé¢æµ‹è¯•ç½‘ç»œè¯·æ±‚
    #                     await crawler.arun(url="https://httpbin.org/get", config=warmup_config)
    #                     await asyncio.sleep(2)

    #                     logger.info("âœ… æ‰©å±•é¢„çƒ­å®Œæˆ")
    #                 except Exception as e:
    #                     logger.warning(f"âš ï¸ æ‰©å±•é¢„çƒ­å¤±è´¥ï¼Œç»§ç»­å°è¯•: {str(e)}")

    #             # æ‰§è¡Œæ­£å¼æŠ“å–
    #             logger.info(f"ğŸš€ å¼€å§‹æŠ“å–: {request.url}")
    #             result = await crawler.arun(url=request.url, config=crawler_config)

    #             # ğŸ†• æ™ºèƒ½é‡è¯•æœºåˆ¶
    #             if extension_path and result.success:
    #                 content_lower = result.markdown.lower() if result.markdown else ""
    #                 paywall_indicators = [
    #                     "subscribe", "sign in", "premium", "subscription", "paywall", "register"]
    #                 detected_indicators = [
    #                     ind for ind in paywall_indicators if ind in content_lower]

    #                 if len(detected_indicators) > 2:  # å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªä»˜è´¹å¢™æŒ‡æ ‡
    #                     logger.info(
    #                         f"ğŸ”„ æ£€æµ‹åˆ°ä»˜è´¹å¢™æŒ‡æ ‡ {detected_indicators}ï¼Œç­‰å¾…åé‡è¯•...")
    #                     await asyncio.sleep(8)  # æ›´é•¿çš„ç­‰å¾…æ—¶é—´

    #                     # é‡è¯•é…ç½®ï¼šæ›´æ¿€è¿›çš„å‚æ•°
    #                     retry_config = CrawlerRunConfig(
    #                         cache_mode=CacheMode.BYPASS,
    #                         page_timeout=90000,  # 1.5åˆ†é’Ÿ
    #                         markdown_generator=crawler_config.markdown_generator,
    #                         wait_for_images=True,
    #                     )

    #                     retry_result = await crawler.arun(url=request.url, config=retry_config)
    #                     if retry_result.success:
    #                         retry_content_lower = retry_result.markdown.lower() if retry_result.markdown else ""
    #                         retry_indicators = [
    #                             ind for ind in paywall_indicators if ind in retry_content_lower]

    #                         # å¦‚æœé‡è¯•åæŒ‡æ ‡å‡å°‘ï¼Œä½¿ç”¨é‡è¯•ç»“æœ
    #                         if len(retry_indicators) < len(detected_indicators):
    #                             result = retry_result
    #                             logger.info(
    #                                 f"ğŸ‰ é‡è¯•æ”¹å–„äº†ç»“æœï¼ŒæŒ‡æ ‡ä» {len(detected_indicators)} å‡å°‘åˆ° {len(retry_indicators)}")
    #                         else:
    #                             logger.info("ğŸ”„ é‡è¯•æœªèƒ½æ”¹å–„ç»“æœï¼Œä½¿ç”¨åŸå§‹ç»“æœ")

    #             if not result.success:
    #                 raise CrawlerException(
    #                     message=getattr(
    #                         result, 'error_message', 'Markdownè·å–å¤±è´¥'),
    #                     error_type="crawl_failed"
    #                 )

    #             # è§£æç»“æœ
    #             title = None
    #             if hasattr(result, 'metadata') and result.metadata:
    #                 title = result.metadata.get('title')

    #             raw_markdown = None
    #             fit_markdown = None

    #             if hasattr(result, 'markdown'):
    #                 if request.format in [MarkdownFormat.RAW, MarkdownFormat.BOTH]:
    #                     raw_markdown = self._extract_raw_markdown(
    #                         result.markdown)

    #                 if request.format in [MarkdownFormat.FIT, MarkdownFormat.BOTH]:
    #                     fit_markdown = self._extract_fit_markdown(
    #                         result.markdown, raw_markdown)

    #             # è®¡ç®—å­—æ•°
    #             word_count = None
    #             if raw_markdown:
    #                 word_count = len(raw_markdown.split())
    #             elif fit_markdown:
    #                 word_count = len(fit_markdown.split())

    #             # ğŸ†• è¯¦ç»†çš„å†…å®¹è´¨é‡åˆ†æå’Œæ—¥å¿—
    #             if extension_path:
    #                 content_to_check = fit_markdown or raw_markdown or ""
    #                 paywall_indicators = [
    #                     "subscribe", "sign in", "premium", "subscription", "paywall", "register"]
    #                 detected_indicators = [
    #                     ind for ind in paywall_indicators if ind in content_to_check.lower()]

    #                 logger.info(f"ğŸ“Š å†…å®¹åˆ†æç»“æœ:")
    #                 logger.info(f"   - å­—æ•°: {word_count}")
    #                 logger.info(f"   - ä»˜è´¹å¢™æŒ‡æ ‡: {detected_indicators}")
    #                 logger.info(
    #                     f"   - çŠ¶æ€ç : {getattr(result, 'status_code', None)}")

    #                 if len(detected_indicators) <= 1:
    #                     logger.info("âœ… å†…å®¹è´¨é‡è‰¯å¥½ï¼Œç–‘ä¼¼æˆåŠŸç»•è¿‡ä»˜è´¹å¢™")
    #                 else:
    #                     logger.warning(
    #                         f"âš ï¸ æ£€æµ‹åˆ°è¾ƒå¤šä»˜è´¹å¢™æŒ‡æ ‡({len(detected_indicators)}ä¸ª)ï¼Œå¯èƒ½æœªå®Œå…¨ç»•è¿‡")

    #             return MarkdownData(
    #                 url=request.url,
    #                 status_code=getattr(result, 'status_code', None),
    #                 raw_markdown=raw_markdown,
    #                 fit_markdown=fit_markdown,
    #                 title=title,
    #                 word_count=word_count
    #             )

    #     except asyncio.TimeoutError:
    #         logger.error(f"Markdownçˆ¬å–è¶…æ—¶: {request.url}")
    #         raise CrawlerException(
    #             message="Markdownè·å–è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
    #             error_type="timeout"
    #         )
    #     except CrawlerException:
    #         raise
    #     except Exception as e:
    #         logger.error(f"Markdownçˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
    #         raise CrawlerException(
    #             message=f"Markdownè·å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
    #             error_type="unexpected"
    #         )

    async def crawl_markdown(self, request: MarkdownRequest) -> MarkdownData:
        """
        ä¸“é—¨è·å–é¡µé¢çš„Markdownå†…å®¹ - ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–ç‰ˆæœ¬

        ä¼˜å…ˆå°è¯•æ— å¤´æ¨¡å¼ï¼Œå¦‚æœæ•ˆæœä¸å¥½åˆ™è‡ªåŠ¨é™çº§åˆ°å¯è§æ¨¡å¼
        """
        try:
            extension_path = self._get_extension_path()

            # ğŸ†• ç”Ÿäº§ç¯å¢ƒç­–ç•¥ï¼šä¼˜å…ˆå°è¯•æ— å¤´æ¨¡å¼
            if extension_path:
                logger.info(f"ğŸ”Œ æ£€æµ‹åˆ°æ‰©å±•ï¼Œå°è¯•æ— å¤´æ¨¡å¼è¿è¡Œ")
                try:
                    return await self._crawl_markdown_with_mode(request, headless=True)
                except Exception as e:
                    # å¦‚æœæ— å¤´æ¨¡å¼å¤±è´¥ï¼Œé™çº§åˆ°å¯è§æ¨¡å¼
                    logger.warning(f"âš ï¸ æ— å¤´æ¨¡å¼å¤±è´¥ï¼Œé™çº§åˆ°å¯è§æ¨¡å¼: {str(e)}")
                    return await self._crawl_markdown_with_mode(request, headless=False)
            else:
                # æ²¡æœ‰æ‰©å±•ï¼Œç›´æ¥ä½¿ç”¨æ— å¤´æ¨¡å¼
                logger.info("ğŸ“„ æ— æ‰©å±•æ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡†æ— å¤´æŠ“å–")
                return await self._crawl_markdown_with_mode(request, headless=True)

        except Exception as e:
            logger.error(f"Markdownçˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"Markdownè·å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    async def _crawl_markdown_with_mode(self, request: MarkdownRequest, headless: bool) -> MarkdownData:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å¼è¿›è¡ŒmarkdownæŠ“å–çš„å†…éƒ¨æ–¹æ³•
        """
        try:
            browser_config = self._create_browser_config(
                request.js_enabled, force_headless=headless)
            crawler_config = self._create_markdown_crawler_config(request)

            extension_path = self._get_extension_path()
            mode_name = "æ— å¤´" if headless else "å¯è§"
            logger.info(f"ğŸš€ å¼€å§‹{mode_name}æ¨¡å¼æŠ“å–: {request.url}")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # ğŸ”§ ç®€åŒ–çš„æ‰©å±•é¢„çƒ­ï¼ˆç§»é™¤chrome://è®¿é—®ï¼‰
                if extension_path and not headless:
                    logger.info("ğŸ”¥ æ‰©å±•é¢„çƒ­ä¸­...")
                    try:
                        # åªä½¿ç”¨ç®€å•çš„HTTPè¯·æ±‚é¢„çƒ­ï¼Œä¸è®¿é—®chrome://
                        warmup_config = CrawlerRunConfig(
                            cache_mode=CacheMode.BYPASS,
                            page_timeout=10000
                        )
                        await crawler.arun(url="https://httpbin.org/get", config=warmup_config)
                        await asyncio.sleep(2)
                        logger.info("âœ… æ‰©å±•é¢„çƒ­å®Œæˆ")
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ‰©å±•é¢„çƒ­å¤±è´¥ï¼Œç»§ç»­å°è¯•: {str(e)}")

                # æ‰§è¡Œæ­£å¼æŠ“å–
                result = await crawler.arun(url=request.url, config=crawler_config)

                # ğŸ”§ æ™ºèƒ½é‡è¯•é€»è¾‘ï¼ˆä»…åœ¨å¯è§æ¨¡å¼æˆ–åˆæ¬¡å¤±è´¥æ—¶ï¼‰
                if extension_path and result.success:
                    content_lower = result.markdown.lower() if result.markdown else ""
                    paywall_indicators = [
                        "subscribe", "sign in", "premium", "subscription", "paywall", "register"]
                    detected_indicators = [
                        ind for ind in paywall_indicators if ind in content_lower]

                    # åªæœ‰åœ¨æ£€æµ‹åˆ°è¾ƒå¤šä»˜è´¹å¢™æŒ‡æ ‡ä¸”æ˜¯å¯è§æ¨¡å¼æ—¶æ‰é‡è¯•
                    if len(detected_indicators) > 2 and not headless:
                        logger.info(
                            f"ğŸ”„ å¯è§æ¨¡å¼æ£€æµ‹åˆ°ä»˜è´¹å¢™æŒ‡æ ‡ {detected_indicators}ï¼Œé‡è¯•ä¸­...")
                        await asyncio.sleep(5)

                        retry_config = CrawlerRunConfig(
                            cache_mode=CacheMode.BYPASS,
                            page_timeout=60000,
                            markdown_generator=crawler_config.markdown_generator,
                            wait_for_images=True,
                        )

                        retry_result = await crawler.arun(url=request.url, config=retry_config)
                        if retry_result.success:
                            retry_content_lower = retry_result.markdown.lower() if retry_result.markdown else ""
                            retry_indicators = [
                                ind for ind in paywall_indicators if ind in retry_content_lower]

                            if len(retry_indicators) < len(detected_indicators):
                                result = retry_result
                                logger.info(
                                    f"ğŸ‰ é‡è¯•æ”¹å–„äº†ç»“æœï¼ŒæŒ‡æ ‡ä» {len(detected_indicators)} å‡å°‘åˆ° {len(retry_indicators)}")

                if not result.success:
                    raise CrawlerException(
                        message=getattr(
                            result, 'error_message', 'Markdownè·å–å¤±è´¥'),
                        error_type="crawl_failed"
                    )

                # è§£æç»“æœ
                title = None
                if hasattr(result, 'metadata') and result.metadata:
                    title = result.metadata.get('title')

                raw_markdown = None
                fit_markdown = None

                if hasattr(result, 'markdown'):
                    if request.format in [MarkdownFormat.RAW, MarkdownFormat.BOTH]:
                        raw_markdown = self._extract_raw_markdown(
                            result.markdown)

                    if request.format in [MarkdownFormat.FIT, MarkdownFormat.BOTH]:
                        fit_markdown = self._extract_fit_markdown(
                            result.markdown, raw_markdown)

                # è®¡ç®—å­—æ•°
                word_count = None
                if raw_markdown:
                    word_count = len(raw_markdown.split())
                elif fit_markdown:
                    word_count = len(fit_markdown.split())

                # ğŸ”§ å†…å®¹è´¨é‡åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
                if extension_path:
                    content_to_check = fit_markdown or raw_markdown or ""
                    paywall_indicators = [
                        "subscribe", "sign in", "premium", "subscription", "paywall"]
                    detected_indicators = [
                        ind for ind in paywall_indicators if ind in content_to_check.lower()]

                    logger.info(
                        f"ğŸ“Š {mode_name}æ¨¡å¼æŠ“å–ç»“æœ: å­—æ•°={word_count}, ä»˜è´¹å¢™æŒ‡æ ‡={len(detected_indicators)}ä¸ª, çŠ¶æ€ç ={getattr(result, 'status_code', None)}")

                    if len(detected_indicators) <= 1:
                        logger.info("âœ… å†…å®¹è´¨é‡è‰¯å¥½")
                    elif len(detected_indicators) <= 3:
                        logger.info("ğŸŸ¡ å†…å®¹è´¨é‡ä¸­ç­‰")
                    else:
                        logger.warning("ğŸ”´ å†…å®¹è´¨é‡è¾ƒå·®ï¼Œå¯èƒ½æœªå®Œå…¨ç»•è¿‡ä»˜è´¹å¢™")

                        # å¦‚æœæ˜¯æ— å¤´æ¨¡å¼ä¸”æ•ˆæœä¸å¥½ï¼ŒæŠ›å‡ºå¼‚å¸¸è§¦å‘é™çº§
                        if headless and len(detected_indicators) > 3:
                            raise CrawlerException(
                                message=f"æ— å¤´æ¨¡å¼æ•ˆæœä¸ä½³ï¼Œæ£€æµ‹åˆ°{len(detected_indicators)}ä¸ªä»˜è´¹å¢™æŒ‡æ ‡",
                                error_type="headless_mode_insufficient"
                            )

                return MarkdownData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    raw_markdown=raw_markdown,
                    fit_markdown=fit_markdown,
                    title=title,
                    word_count=word_count
                )

        except asyncio.TimeoutError:
            logger.error(f"{mode_name}æ¨¡å¼Markdownçˆ¬å–è¶…æ—¶: {request.url}")
            raise CrawlerException(
                message=f"{mode_name}æ¨¡å¼Markdownè·å–è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                error_type="timeout"
            )

    # ğŸ†• ä¸ºè°ƒè¯•æ¥å£æä¾›ä¸“é—¨çš„æ–¹æ³•

    async def crawl_markdown_debug(self, request: MarkdownRequest) -> MarkdownData:
        """
        è°ƒè¯•ä¸“ç”¨çš„markdownæŠ“å– - å¼ºåˆ¶å¯è§æ¨¡å¼ï¼Œè¯¦ç»†æ—¥å¿—
        """
        browser_config = self._create_browser_config(
            request.js_enabled, force_headless=False)
        crawler_config = self._create_markdown_crawler_config(request)

        extension_path = self._get_extension_path()
        logger.info(f"ğŸ” è°ƒè¯•æ¨¡å¼æŠ“å–: {request.url}")

        async with AsyncWebCrawler(config=browser_config) as crawler:
            # è°ƒè¯•æ¨¡å¼çš„è¯¦ç»†é¢„çƒ­
            if extension_path:
                logger.info("ğŸ”¥ è°ƒè¯•æ¨¡å¼æ‰©å±•é¢„çƒ­...")
                try:
                    warmup_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=10000
                    )
                    await crawler.arun(url="https://httpbin.org/get", config=warmup_config)
                    await asyncio.sleep(3)
                    logger.info("âœ… è°ƒè¯•é¢„çƒ­å®Œæˆ")
                except Exception as e:
                    logger.warning(f"âš ï¸ è°ƒè¯•é¢„çƒ­å¤±è´¥: {str(e)}")

            # æ‰§è¡ŒæŠ“å–
            result = await crawler.arun(url=request.url, config=crawler_config)

            if not result.success:
                raise CrawlerException(
                    message=getattr(result, 'error_message', 'è°ƒè¯•Markdownè·å–å¤±è´¥'),
                    error_type="crawl_failed"
                )

            # ç®€åŒ–çš„ç»“æœè§£æ
            raw_markdown = result.markdown if hasattr(
                result, 'markdown') else None
            word_count = len(raw_markdown.split()) if raw_markdown else 0

            logger.info(
                f"ğŸ” è°ƒè¯•æŠ“å–å®Œæˆ: å­—æ•°={word_count}, çŠ¶æ€ç ={getattr(result, 'status_code', None)}")

            return MarkdownData(
                url=request.url,
                status_code=getattr(result, 'status_code', None),
                raw_markdown=raw_markdown,
                fit_markdown=raw_markdown,  # è°ƒè¯•æ¨¡å¼ç®€åŒ–å¤„ç†
                title=getattr(result, 'metadata', {}).get(
                    'title') if hasattr(result, 'metadata') else None,
                word_count=word_count
            )

    @staticmethod
    def _extract_raw_markdown(markdown_result) -> str:
        """æå–åŸå§‹markdownå†…å®¹"""
        if hasattr(markdown_result, 'raw_markdown'):
            return markdown_result.raw_markdown
        else:
            # å‘åå…¼å®¹ï¼Œå¦‚æœæ²¡æœ‰raw_markdownå±æ€§ï¼Œä½¿ç”¨markdownæœ¬èº«
            return markdown_result if isinstance(markdown_result, str) else str(markdown_result)

    @staticmethod
    def _extract_fit_markdown(markdown_result, raw_markdown: str = None) -> str:
        """æå–ç»è¿‡è¿‡æ»¤çš„markdownå†…å®¹"""
        if hasattr(markdown_result, 'fit_markdown'):
            return markdown_result.fit_markdown
        else:
            # å¦‚æœæ²¡æœ‰fit_markdownï¼Œä½¿ç”¨raw_markdownä½œä¸ºå¤‡é€‰
            return raw_markdown

    @staticmethod
    def _create_screenshot_browser_config(request: ScreenshotRequest) -> BrowserConfig:
        """åˆ›å»ºæˆªå›¾ä¸“ç”¨æµè§ˆå™¨é…ç½®"""
        return BrowserConfig(
            headless=True,
            java_script_enabled=request.js_enabled,
            viewport={"width": request.viewport_width or 1280,
                      "height": request.viewport_height or 720},
            verbose=False
        )

    @staticmethod
    def _create_screenshot_crawler_config(request: ScreenshotRequest) -> CrawlerRunConfig:
        """åˆ›å»ºæˆªå›¾ä¸“ç”¨çˆ¬è™«é…ç½®"""
        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
            page_timeout=60000,  # 60ç§’
            screenshot=True,
        )

        if request.css_selector:
            config.css_selector = request.css_selector

        if request.wait_for:
            config.wait_until = request.wait_for

        return config

    async def take_screenshot(self, request: ScreenshotRequest) -> ScreenshotData:
        """
        æˆªå–é¡µé¢æˆªå›¾ - è¿”å›çº¯ä¸šåŠ¡æ•°æ®æˆ–æŠ›å‡ºå¼‚å¸¸

        Args:
            request: æˆªå›¾è¯·æ±‚å¯¹è±¡

        Returns:
            ScreenshotData: æˆªå›¾ä¸šåŠ¡æ•°æ®

        Raises:
            CrawlerException: æˆªå›¾å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            browser_config = self._create_screenshot_browser_config(request)
            crawler_config = self._create_screenshot_crawler_config(request)

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=crawler_config)

                if not result.success:
                    raise CrawlerException(
                        message=getattr(result, 'error_message', 'æˆªå›¾å¤±è´¥'),
                        error_type="screenshot_failed"
                    )

                if not result.screenshot:
                    raise CrawlerException(
                        message="æˆªå›¾æ•°æ®ä¸ºç©º",
                        error_type="screenshot_empty"
                    )

                # è¿”å›çº¯ä¸šåŠ¡æ•°æ®
                return ScreenshotData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    screenshot_base64=result.screenshot,
                    error_message=None
                )

        except asyncio.TimeoutError:
            logger.error(f"æˆªå›¾è¶…æ—¶: {request.url}")
            raise CrawlerException(
                message="æˆªå›¾è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                error_type="timeout"
            )
        except CrawlerException:
            # é‡æ–°æŠ›å‡ºå·²çŸ¥å¼‚å¸¸
            raise
        except Exception as e:
            logger.error(f"æˆªå›¾å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"æˆªå›¾è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )


# åˆ›å»ºæœåŠ¡å®ä¾‹
crawler_service = CrawlerService()

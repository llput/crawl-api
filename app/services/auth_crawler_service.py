# app/services/auth_crawler_service.py
import asyncio
import logging
import os
import glob
from pathlib import Path
from typing import Dict, Optional

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from app.models.models import CrawlRequest, CrawlData, MarkdownRequest, MarkdownData
from app.services.crawler_service import CrawlerException

logger = logging.getLogger(__name__)


class AuthCrawlerService:
    """å¸¦è®¤è¯åŠŸèƒ½çš„çˆ¬è™«æœåŠ¡"""

    def __init__(self):
        # è®¤è¯é…ç½®æ–‡ä»¶å­˜å‚¨ç›®å½•
        self.auth_profiles_dir = Path("./auth_profiles")
        self.auth_profiles_dir.mkdir(parents=True, exist_ok=True)

    def get_profile_path(self, site_name: str) -> str:
        """è·å–æŒ‡å®šç«™ç‚¹çš„è®¤è¯é…ç½®æ–‡ä»¶è·¯å¾„"""
        return str((self.auth_profiles_dir / site_name).resolve())

    def _get_extension_path(self) -> Optional[str]:
        """è·å–æ‰©å±•è·¯å¾„"""
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®
        env_path = os.environ.get('CHROME_EXTENSION_PATH')
        if env_path and os.path.exists(env_path):
            return env_path

        # é»˜è®¤æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ chrome-extension æ–‡ä»¶å¤¹
        project_extension_path = Path(
            "./chrome-extension/bypass-paywalls-chrome-clean")
        if project_extension_path.exists():
            return str(project_extension_path.resolve())

        # å¤‡é€‰è·¯å¾„ 1: download æ–‡ä»¶å¤¹
        download_path = Path("./download/bypass-paywalls-chrome-clean-master")
        if download_path.exists():
            return str(download_path.resolve())

        # å¤‡é€‰è·¯å¾„ 2: ç”¨æˆ·ä¸‹è½½ç›®å½•
        home_download_path = Path.home() / "Downloads" / \
            "bypass-paywalls-chrome-clean-master"
        if home_download_path.exists():
            return str(home_download_path.resolve())

        return None

    def _create_auth_browser_config(
        self,
        site_name: str,
        js_enabled: bool = True,
        headless: bool = True
    ) -> BrowserConfig:
        """åˆ›å»ºå¸¦è®¤è¯çš„æµè§ˆå™¨é…ç½®"""
        user_data_dir = self.get_profile_path(site_name)

        # å¼ºåˆ¶è·å–æµè§ˆå™¨è·¯å¾„
        browser_path = self._get_browser_executable_path()

        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰©å±•éœ€è¦åŠ è½½
        extension_path = self._get_extension_path()

        # å¦‚æœæœ‰æ‰©å±•ï¼Œå¼ºåˆ¶ä½¿ç”¨éæ— å¤´æ¨¡å¼
        if extension_path:
            headless = False
            logger.info(f"ğŸ”Œ æ£€æµ‹åˆ°æ‰©å±•ï¼Œå°†ä½¿ç”¨éæ— å¤´æ¨¡å¼: {extension_path}")

        # åˆ›å»ºæµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=headless,
            java_script_enabled=js_enabled,
            use_persistent_context=True,
            user_data_dir=user_data_dir,
            viewport={"width": 1280, "height": 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            verbose=True
        )

        # å¼ºåˆ¶è®¾ç½®æµè§ˆå™¨è·¯å¾„
        if browser_path:
            browser_config.browser_executable_path = browser_path
            logger.info(f"âœ… å¼ºåˆ¶è®¾ç½®æµè§ˆå™¨è·¯å¾„: {browser_path}")
        else:
            # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å·²çŸ¥è·¯å¾„
            fallback_path = "/Users/M16/Library/Caches/ms-playwright/chromium-1169/chrome-mac/Chromium.app/Contents/MacOS/Chromium"
            if os.path.exists(fallback_path):
                browser_config.browser_executable_path = fallback_path
                logger.info(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨è·¯å¾„: {fallback_path}")
            else:
                logger.error("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æµè§ˆå™¨è·¯å¾„")
                raise CrawlerException(
                    message=f"æœªæ‰¾åˆ°å¯ç”¨çš„ Chromium æµè§ˆå™¨ã€‚è¯·ç¡®ä¿å·²å®‰è£… Playwright æµè§ˆå™¨æˆ–è®¾ç½®æ­£ç¡®çš„æµè§ˆå™¨è·¯å¾„",
                    error_type="browser_not_found"
                )

        # ğŸ†• æ·»åŠ æ‰©å±•æ”¯æŒ
        if extension_path:
            # æ·»åŠ æ‰©å±•ç›¸å…³å‚æ•°åˆ° extra_args
            extension_args = [
                f"--load-extension={extension_path}",
                f"--disable-extensions-except={extension_path}",
                "--disable-extensions-except-devtools"  # å…è®¸å¼€å‘è€…å·¥å…·æ‰©å±•
            ]

            # åˆå§‹åŒ– extra_args å¦‚æœä¸å­˜åœ¨
            if not hasattr(browser_config, 'extra_args') or browser_config.extra_args is None:
                browser_config.extra_args = []

            browser_config.extra_args.extend(extension_args)
            logger.info(f"ğŸ”Œ å·²æ·»åŠ æ‰©å±•å‚æ•°: {extension_args}")

        # éªŒè¯è·¯å¾„æ˜¯å¦å¯æ‰§è¡Œ
        if hasattr(browser_config, 'browser_executable_path') and browser_config.browser_executable_path:
            if not os.access(browser_config.browser_executable_path, os.X_OK):
                logger.warning(
                    f"âš ï¸ æµè§ˆå™¨æ–‡ä»¶ä¸å¯æ‰§è¡Œï¼Œå°è¯•ä¿®å¤æƒé™: {browser_config.browser_executable_path}")
                try:
                    os.chmod(browser_config.browser_executable_path, 0o755)
                    logger.info("âœ… æƒé™ä¿®å¤æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ æƒé™ä¿®å¤å¤±è´¥: {e}")

        return browser_config

    def _get_browser_executable_path(self) -> Optional[str]:
        """è·å–æµè§ˆå™¨å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„"""

        # 1. ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
        env_path = os.environ.get('CHROMIUM_EXECUTABLE_PATH')
        if env_path and os.path.exists(env_path):
            logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æµè§ˆå™¨: {env_path}")
            return env_path

        # 2. ä»é…ç½®æ–‡ä»¶è¯»å–
        config_file = Path("./browser_config.txt")
        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    path = f.read().strip()
                    if path and os.path.exists(path):
                        logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶æŒ‡å®šçš„æµè§ˆå™¨: {path}")
                        return path
            except Exception:
                pass

        # 3. è‡ªåŠ¨æ£€æµ‹
        auto_path = self._auto_detect_chromium()
        if auto_path:
            logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°æµè§ˆå™¨: {auto_path}")
            return auto_path

        # 4. å°è¯•ç”¨æˆ·æŠ¥å‘Šçš„å…·ä½“è·¯å¾„
        user_reported_path = "/Users/M16/Library/Caches/ms-playwright/chromium-1169/chrome-mac/Chromium.app/Contents/MacOS/Chromium"
        if os.path.exists(user_reported_path):
            logger.info(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„æµè§ˆå™¨è·¯å¾„: {user_reported_path}")
            return user_reported_path

        logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„æµè§ˆå™¨è·¯å¾„")
        return None

    def _auto_detect_chromium(self) -> Optional[str]:
        """è‡ªåŠ¨æ£€æµ‹ Chromium è·¯å¾„"""
        import platform

        system = platform.system()

        if system == "Darwin":  # macOS
            patterns = [
                "/Users/*/Library/Caches/ms-playwright/chromium-*/chrome-mac/Chromium.app/Contents/MacOS/Chromium",
                "/Applications/Chromium.app/Contents/MacOS/Chromium",
                "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
            ]
        elif system == "Linux":
            patterns = [
                "/home/*/snap/chromium/*/usr/lib/chromium-browser/chrome",
                "/usr/bin/chromium-browser",
                "/usr/bin/google-chrome"
            ]
        elif system == "Windows":
            patterns = [
                "C:/Users/*/AppData/Local/ms-playwright/chromium-*/chrome-win/chrome.exe",
                "C:/Program Files/Google/Chrome/Application/chrome.exe"
            ]
        else:
            return None

        for pattern in patterns:
            matches = glob.glob(pattern)
            if matches:
                # é€‰æ‹©ç‰ˆæœ¬å·æœ€é«˜çš„
                latest = sorted(matches, reverse=True)[0]
                if os.path.exists(latest):
                    return latest

        return None

    def _create_auth_browser_config(
        self,
        site_name: str,
        js_enabled: bool = True,
        headless: bool = True
    ) -> BrowserConfig:
        """åˆ›å»ºå¸¦è®¤è¯çš„æµè§ˆå™¨é…ç½®"""
        user_data_dir = self.get_profile_path(site_name)

        # å¼ºåˆ¶è·å–æµè§ˆå™¨è·¯å¾„
        browser_path = self._get_browser_executable_path()

        # åˆ›å»ºæµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=headless,
            java_script_enabled=js_enabled,
            use_persistent_context=True,
            user_data_dir=user_data_dir,
            viewport={"width": 1280, "height": 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            verbose=True
        )

        # å¼ºåˆ¶è®¾ç½®æµè§ˆå™¨è·¯å¾„
        if browser_path:
            browser_config.browser_executable_path = browser_path
            logger.info(f"âœ… å¼ºåˆ¶è®¾ç½®æµè§ˆå™¨è·¯å¾„: {browser_path}")
        else:
            # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å·²çŸ¥è·¯å¾„
            fallback_path = "/Users/M16/Library/Caches/ms-playwright/chromium-1169/chrome-mac/Chromium.app/Contents/MacOS/Chromium"
            if os.path.exists(fallback_path):
                browser_config.browser_executable_path = fallback_path
                logger.info(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨è·¯å¾„: {fallback_path}")
            else:
                logger.error("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æµè§ˆå™¨è·¯å¾„")
                raise CrawlerException(
                    message=f"æœªæ‰¾åˆ°å¯ç”¨çš„ Chromium æµè§ˆå™¨ã€‚è¯·ç¡®ä¿å·²å®‰è£… Playwright æµè§ˆå™¨æˆ–è®¾ç½®æ­£ç¡®çš„æµè§ˆå™¨è·¯å¾„",
                    error_type="browser_not_found"
                )

        # éªŒè¯è·¯å¾„æ˜¯å¦å¯æ‰§è¡Œ
        if hasattr(browser_config, 'browser_executable_path') and browser_config.browser_executable_path:
            if not os.access(browser_config.browser_executable_path, os.X_OK):
                logger.warning(
                    f"âš ï¸ æµè§ˆå™¨æ–‡ä»¶ä¸å¯æ‰§è¡Œï¼Œå°è¯•ä¿®å¤æƒé™: {browser_config.browser_executable_path}")
                try:
                    os.chmod(browser_config.browser_executable_path, 0o755)
                    logger.info("âœ… æƒé™ä¿®å¤æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ æƒé™ä¿®å¤å¤±è´¥: {e}")

        return browser_config

    async def setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        setup_timeout: int = 300
    ) -> Dict[str, str]:
        """
        è®¾ç½®è®¤è¯é…ç½®æ–‡ä»¶ - æ‰“å¼€å¯è§æµè§ˆå™¨ä¾›æ‰‹åŠ¨ç™»å½•

        Args:
            site_name: ç«™ç‚¹åç§°ï¼Œç”¨ä½œé…ç½®æ–‡ä»¶å
            login_url: ç™»å½•é¡µé¢URL
            test_url: ç”¨äºæµ‹è¯•ç™»å½•çŠ¶æ€çš„URL
            setup_timeout: è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Dict: åŒ…å«æ“ä½œç»“æœçš„å­—å…¸
        """
        try:
            # åˆ›å»ºå¯è§æµè§ˆå™¨é…ç½®ï¼ˆç”¨äºæ‰‹åŠ¨ç™»å½•ï¼‰
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False  # å¯è§æ¨¡å¼
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=setup_timeout * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            )

            logger.info(f"æ­£åœ¨ä¸º {site_name} è®¾ç½®è®¤è¯é…ç½®æ–‡ä»¶...")
            logger.info(f"å°†æ‰“å¼€æµè§ˆå™¨çª—å£ï¼Œè¯·æ‰‹åŠ¨å®Œæˆç™»å½•è¿‡ç¨‹")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # å…ˆè®¿é—®ç™»å½•é¡µé¢
                logger.info(f"è®¿é—®ç™»å½•é¡µé¢: {login_url}")
                login_result = await crawler.arun(url=login_url, config=config)

                if not login_result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•è®¿é—®ç™»å½•é¡µé¢: {login_result.error_message}",
                        error_type="setup_failed"
                    )

                # è®¿é—®æµ‹è¯•é¡µé¢ä»¥éªŒè¯ç™»å½•çŠ¶æ€å¹¶ä¿å­˜ä¼šè¯
                logger.info(f"è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åè„šæœ¬å°†éªŒè¯ç™»å½•çŠ¶æ€...")
                logger.info(f"éªŒè¯ç™»å½•çŠ¶æ€: {test_url}")

                test_result = await crawler.arun(url=test_url, config=config)

                if test_result.success:
                    # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸçš„ç®€å•éªŒè¯
                    content_lower = test_result.html.lower()
                    if any(keyword in content_lower for keyword in ['login', 'sign in', 'signin']) and \
                       not any(keyword in content_lower for keyword in ['logout', 'sign out', 'account', 'profile']):
                        logger.warning("å¯èƒ½ä»æœªç™»å½•ï¼Œè¯·æ£€æŸ¥ç™»å½•çŠ¶æ€")
                        return {
                            "status": "warning",
                            "message": "è®¤è¯é…ç½®å·²ä¿å­˜ï¼Œä½†å¯èƒ½æœªæˆåŠŸç™»å½•ï¼Œè¯·æ£€æŸ¥",
                            "profile_path": self.get_profile_path(site_name)
                        }
                    else:
                        logger.info("è®¤è¯é…ç½®è®¾ç½®æˆåŠŸï¼")
                        return {
                            "status": "success",
                            "message": "è®¤è¯é…ç½®è®¾ç½®æˆåŠŸï¼Œä¼šè¯å·²ä¿å­˜",
                            "profile_path": self.get_profile_path(site_name)
                        }
                else:
                    raise CrawlerException(
                        message=f"éªŒè¯ç™»å½•çŠ¶æ€å¤±è´¥: {test_result.error_message}",
                        error_type="setup_failed"
                    )

        except Exception as e:
            logger.error(f"è®¾ç½®è®¤è¯é…ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"è®¾ç½®è®¤è¯é…ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    async def crawl_with_auth(
        self,
        site_name: str,
        request: CrawlRequest
    ) -> CrawlData:
        """
        ä½¿ç”¨ä¿å­˜çš„è®¤è¯é…ç½®çˆ¬å–URL

        Args:
            site_name: ç«™ç‚¹åç§°
            request: çˆ¬å–è¯·æ±‚

        Returns:
            CrawlData: çˆ¬å–ç»“æœ
        """
        try:
            # æ£€æŸ¥è®¤è¯é…ç½®æ˜¯å¦å­˜åœ¨
            profile_path = self.get_profile_path(site_name)
            if not os.path.exists(profile_path):
                raise CrawlerException(
                    message=f"è®¤è¯é…ç½®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè°ƒç”¨ setup_auth_profile è®¾ç½®è®¤è¯",
                    error_type="auth_required"
                )

            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                js_enabled=request.js_enabled,
                headless=True  # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æ— å¤´æ¨¡å¼
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
                page_timeout=60000,
                wait_for_images=request.include_images,
            )

            if request.css_selector:
                config.css_selector = request.css_selector

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=config)

                if not result.success:
                    # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°ç™»å½•é¡µé¢
                    if result.status_code in [401, 403] or \
                       any(keyword in result.html.lower() for keyword in ['login', 'sign in', 'signin']):
                        raise CrawlerException(
                            message="è®¤è¯å·²è¿‡æœŸï¼Œè¯·é‡æ–°è®¾ç½®è®¤è¯é…ç½®",
                            error_type="auth_expired"
                        )
                    else:
                        raise CrawlerException(
                            message=getattr(result, 'error_message', 'çˆ¬å–å¤±è´¥'),
                            error_type="crawl_failed"
                        )

                return CrawlData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    markdown=result.markdown,
                    media=result.media if hasattr(result, 'media') else None,
                    links=result.links if hasattr(result, 'links') else None
                )

        except CrawlerException:
            raise
        except Exception as e:
            logger.error(f"è®¤è¯çˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"è®¤è¯çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    async def crawl_markdown_with_auth(
        self,
        site_name: str,
        request: MarkdownRequest
    ) -> MarkdownData:
        """
        ä½¿ç”¨ä¿å­˜çš„è®¤è¯é…ç½®è·å–Markdown

        Args:
            site_name: ç«™ç‚¹åç§°
            request: Markdownè¯·æ±‚

        Returns:
            MarkdownData: Markdownæ•°æ®
        """
        try:
            # æ£€æŸ¥è®¤è¯é…ç½®æ˜¯å¦å­˜åœ¨
            profile_path = self.get_profile_path(site_name)
            if not os.path.exists(profile_path):
                raise CrawlerException(
                    message=f"è®¤è¯é…ç½®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®¾ç½®è®¤è¯",
                    error_type="auth_required"
                )

            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                js_enabled=request.js_enabled,
                headless=True
            )

            # åˆ›å»ºMarkdownä¸“ç”¨é…ç½®ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥å‚è€ƒåŸæœåŠ¡çš„markdowné…ç½®ï¼‰
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
                page_timeout=60000,
            )

            if request.css_selector:
                config.css_selector = request.css_selector

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=config)

                if not result.success:
                    if result.status_code in [401, 403] or \
                       any(keyword in result.html.lower() for keyword in ['login', 'sign in']):
                        raise CrawlerException(
                            message="è®¤è¯å·²è¿‡æœŸï¼Œè¯·é‡æ–°è®¾ç½®è®¤è¯é…ç½®",
                            error_type="auth_expired"
                        )
                    else:
                        raise CrawlerException(
                            message=getattr(
                                result, 'error_message', 'Markdownè·å–å¤±è´¥'),
                            error_type="crawl_failed"
                        )

                # è§£æç»“æœ
                title = None
                if hasattr(result, 'metadata') and result.metadata:
                    title = result.metadata.get('title')

                raw_markdown = result.markdown if hasattr(
                    result, 'markdown') else None
                word_count = len(raw_markdown.split()) if raw_markdown else 0

                return MarkdownData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    raw_markdown=raw_markdown,
                    fit_markdown=raw_markdown,  # ç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ·»åŠ è¿‡æ»¤é€»è¾‘
                    title=title,
                    word_count=word_count
                )

        except CrawlerException:
            raise
        except Exception as e:
            logger.error(f"è®¤è¯Markdownçˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"è®¤è¯Markdownè·å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    def list_auth_profiles(self) -> Dict[str, Dict]:
        """åˆ—å‡ºæ‰€æœ‰å·²è®¾ç½®çš„è®¤è¯é…ç½®"""
        profiles = {}

        for profile_dir in self.auth_profiles_dir.iterdir():
            if profile_dir.is_dir():
                profiles[profile_dir.name] = {
                    "site_name": profile_dir.name,
                    "profile_path": str(profile_dir.resolve()),
                    "created_time": profile_dir.stat().st_mtime
                }

        return profiles

    async def debug_setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        setup_timeout: int = 300
    ) -> Dict[str, str]:
        """
        è°ƒè¯•ç‰ˆè®¤è¯è®¾ç½® - ç”¨äºæ’æŸ¥æµè§ˆå™¨å…³é—­é—®é¢˜
        """
        try:
            # å‚æ•°è§„èŒƒåŒ–
            if setup_timeout > 1000:
                setup_timeout = setup_timeout // 1000
            setup_timeout = max(60, min(setup_timeout, 600))

            logger.info(f"ğŸ”§ è°ƒè¯•æ¨¡å¼ - è®¾ç½®è¶…æ—¶: {setup_timeout} ç§’")

            # åˆ›å»ºæµè§ˆå™¨é…ç½®
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False
            )

            logger.info("ğŸŒ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                logger.info("âœ… æµè§ˆå™¨å·²å¯åŠ¨")

                try:
                    # ç¬¬ä¸€æ­¥ï¼šå°è¯•æ‰“å¼€ç™»å½•é¡µé¢
                    logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€: {login_url}")

                    config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=30000  # 30ç§’
                    )

                    result = await crawler.arun(url=login_url, config=config)

                    if result.success:
                        logger.info("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
                        logger.info("ğŸš¨ æµè§ˆå™¨åº”è¯¥ç°åœ¨æ˜¯æ‰“å¼€çŠ¶æ€ï¼Œè¯·æ£€æŸ¥ï¼")

                        # ç®€å•ç­‰å¾… - ä¸åšä»»ä½•å¤æ‚æ“ä½œ
                        wait_time = min(setup_timeout, 300)  # æœ€å¤šç­‰5åˆ†é’Ÿ
                        logger.info(f"â° å¼€å§‹ç­‰å¾… {wait_time} ç§’...")

                        # åˆ†æ®µç­‰å¾…ï¼Œæ¯30ç§’æŠ¥å‘Šä¸€æ¬¡
                        for i in range(0, wait_time, 30):
                            remaining = wait_time - i
                            logger.info(f"â° å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining} ç§’")
                            logger.info("   è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•...")
                            await asyncio.sleep(min(30, remaining))

                        logger.info("âœ… ç­‰å¾…å®Œæˆï¼Œå‡†å¤‡éªŒè¯")

                        # éªŒè¯æ­¥éª¤
                        if test_url != login_url:
                            logger.info(f"ğŸ” éªŒè¯ç™»å½•çŠ¶æ€: {test_url}")
                            verify_result = await crawler.arun(url=test_url, config=config)

                            if verify_result.success:
                                logger.info("âœ… éªŒè¯é¡µé¢è®¿é—®æˆåŠŸ")
                                return {
                                    "status": "success",
                                    "message": "è°ƒè¯•æ¨¡å¼å®Œæˆ - è¯·æ£€æŸ¥å®é™…ç™»å½•çŠ¶æ€",
                                    "profile_path": self.get_profile_path(site_name)
                                }
                            else:
                                logger.error(
                                    f"âŒ éªŒè¯é¡µé¢è®¿é—®å¤±è´¥: {verify_result.error_message}")
                        else:
                            logger.info("ğŸ”„ æµ‹è¯•URLä¸ç™»å½•URLç›¸åŒï¼Œè·³è¿‡éªŒè¯")
                            return {
                                "status": "warning",
                                "message": "è°ƒè¯•æ¨¡å¼å®Œæˆ - å»ºè®®ä½¿ç”¨ä¸åŒçš„æµ‹è¯•URL",
                                "profile_path": self.get_profile_path(site_name)
                            }

                    else:
                        logger.error(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {result.error_message}")
                        raise CrawlerException(
                            message=f"æ— æ³•æ‰“å¼€ç™»å½•é¡µé¢: {result.error_message}",
                            error_type="setup_failed"
                        )

                except Exception as e:
                    logger.error(f"âŒ å†…éƒ¨å¼‚å¸¸: {str(e)}")
                    # å³ä½¿æœ‰å¼‚å¸¸ä¹Ÿè¦ç­‰å¾…ï¼Œè®©ç”¨æˆ·çœ‹åˆ°æµè§ˆå™¨
                    logger.info("ğŸš¨ å‡ºç°å¼‚å¸¸ä½†ç»§ç»­ç­‰å¾…ï¼Œè®©æ‚¨æ£€æŸ¥æµè§ˆå™¨çŠ¶æ€...")
                    await asyncio.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
                    raise

            logger.info("ğŸ”š æµè§ˆå™¨å³å°†å…³é—­")

        except Exception as e:
            logger.error(f"è®¾ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"è°ƒè¯•è®¾ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    def delete_auth_profile(self, site_name: str) -> bool:
        """åˆ é™¤æŒ‡å®šçš„è®¤è¯é…ç½®"""
        import shutil

        profile_path = Path(self.get_profile_path(site_name))
        if profile_path.exists():
            shutil.rmtree(profile_path)
            logger.info(f"å·²åˆ é™¤è®¤è¯é…ç½®: {site_name}")
            return True
        return False

    async def manual_setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        setup_timeout: int = 600
    ) -> Dict[str, str]:
        """
        æ‰‹åŠ¨æ§åˆ¶ç‰ˆè®¤è¯è®¾ç½® - ç”¨æˆ·é€šè¿‡APIæ§åˆ¶æµç¨‹
        """
        try:
            # åˆ›å»ºæµè§ˆå™¨é…ç½®
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False
            )

            logger.info(f"ğŸš€ å¼€å§‹æ‰‹åŠ¨æ§åˆ¶è®¤è¯è®¾ç½®: {site_name}")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # ç¬¬ä¸€æ­¥ï¼šæ‰“å¼€ç™»å½•é¡µé¢
                logger.info(f"ğŸ“– æ‰“å¼€ç™»å½•é¡µé¢: {login_url}")

                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    page_timeout=30000
                )

                result = await crawler.arun(url=login_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•æ‰“å¼€ç™»å½•é¡µé¢: {result.error_message}",
                        error_type="setup_failed"
                    )

                logger.info("âœ… æµè§ˆå™¨å·²æ‰“å¼€ï¼Œç™»å½•é¡µé¢åŠ è½½å®Œæˆ")
                logger.info("=" * 60)
                logger.info("ğŸ”‘ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åï¼š")
                logger.info("   1. è°ƒç”¨ /api/v1/auth-crawl/verify-login éªŒè¯ç™»å½•")
                logger.info("   2. æˆ–è°ƒç”¨ /api/v1/auth-crawl/close-browser å…³é—­æµè§ˆå™¨")
                logger.info("=" * 60)

                # ä¿æŒæµè§ˆå™¨æ‰“å¼€ï¼Œç­‰å¾…ç”¨æˆ·è°ƒç”¨å…¶ä»–API
                # ä½¿ç”¨ä¸€ä¸ªé•¿æ—¶é—´çš„ç­‰å¾…ï¼Œä½†ä¸åšä»»ä½•æ“ä½œ
                wait_time = min(setup_timeout, 1800)  # æœ€å¤š30åˆ†é’Ÿ
                logger.info(f"â° æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€ {wait_time//60} åˆ†é’Ÿ")

                # åˆ›å»ºä¸€ä¸ªæ ‡è®°æ–‡ä»¶è¡¨ç¤ºæµè§ˆå™¨æ­£åœ¨è¿è¡Œ
                browser_flag_file = Path(
                    f"./auth_profiles/{site_name}_browser_active")
                browser_flag_file.parent.mkdir(parents=True, exist_ok=True)
                browser_flag_file.write_text("active")

                try:
                    # å®šæœŸæ£€æŸ¥æ ‡è®°æ–‡ä»¶ï¼Œå¦‚æœè¢«åˆ é™¤å°±é€€å‡º
                    for i in range(0, wait_time, 10):
                        if not browser_flag_file.exists():
                            logger.info("ğŸ›‘ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå‡†å¤‡å…³é—­æµè§ˆå™¨")
                            break

                        remaining = wait_time - i
                        if remaining % 300 == 0:  # æ¯5åˆ†é’Ÿæé†’ä¸€æ¬¡
                            logger.info(f"â° æµè§ˆå™¨ä»ç„¶æ‰“å¼€ï¼Œå‰©ä½™ {remaining//60} åˆ†é’Ÿ")

                        await asyncio.sleep(10)

                    # æ¸…ç†æ ‡è®°æ–‡ä»¶
                    if browser_flag_file.exists():
                        browser_flag_file.unlink()

                    # æœ€åéªŒè¯ä¸€æ¬¡ç™»å½•çŠ¶æ€
                    logger.info(f"ğŸ” æœ€ç»ˆéªŒè¯ç™»å½•çŠ¶æ€: {test_url}")
                    verify_result = await crawler.arun(url=test_url, config=config)

                    if verify_result.success:
                        analysis = self._analyze_login_status(
                            verify_result, site_name)
                        logger.info(f"ğŸ“Š æœ€ç»ˆç™»å½•çŠ¶æ€: {analysis['status']}")
                        return analysis
                    else:
                        return {
                            "status": "warning",
                            "message": "æµè§ˆå™¨ä¼šè¯å·²ä¿å­˜ï¼Œä½†æ— æ³•éªŒè¯æœ€ç»ˆç™»å½•çŠ¶æ€",
                            "profile_path": self.get_profile_path(site_name)
                        }

                finally:
                    # ç¡®ä¿æ¸…ç†æ ‡è®°æ–‡ä»¶
                    if browser_flag_file.exists():
                        browser_flag_file.unlink()

            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
            return {
                "status": "completed",
                "message": "æ‰‹åŠ¨è®¤è¯è®¾ç½®æµç¨‹å®Œæˆ",
                "profile_path": self.get_profile_path(site_name)
            }

        except Exception as e:
            logger.error(f"æ‰‹åŠ¨è®¤è¯è®¾ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"æ‰‹åŠ¨è®¤è¯è®¾ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    async def verify_login_status(self, site_name: str, test_url: str) -> Dict[str, str]:
        """éªŒè¯å½“å‰ç™»å½•çŠ¶æ€ - ä¸å…³é—­æµè§ˆå™¨"""
        try:
            # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            browser_flag_file = Path(
                f"./auth_profiles/{site_name}_browser_active")
            if not browser_flag_file.exists():
                return {
                    "status": "error",
                    "message": "æµè§ˆå™¨ä¼šè¯ä¸å­˜åœ¨æˆ–å·²å…³é—­"
                }

            # ä½¿ç”¨ç°æœ‰ä¼šè¯éªŒè¯
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=True  # åå°éªŒè¯
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=30000
            )

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=test_url, config=config)

                if result.success:
                    return self._analyze_login_status(result, site_name)
                else:
                    return {
                        "status": "error",
                        "message": f"éªŒè¯å¤±è´¥: {result.error_message}"
                    }

        except Exception as e:
            return {
                "status": "error",
                "message": f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            }

    async def close_browser_session(self, site_name: str) -> Dict[str, str]:
        """å…³é—­æµè§ˆå™¨ä¼šè¯"""
        try:
            browser_flag_file = Path(
                f"./auth_profiles/{site_name}_browser_active")
            if browser_flag_file.exists():
                browser_flag_file.unlink()
                return {
                    "status": "success",
                    "message": "æµè§ˆå™¨å…³é—­ä¿¡å·å·²å‘é€"
                }
            else:
                return {
                    "status": "warning",
                    "message": "æµè§ˆå™¨ä¼šè¯ä¸å­˜åœ¨æˆ–å·²å…³é—­"
                }
        except Exception as e:
            return {
                "status": "error",
                "message": f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {str(e)}"
            }

    async def simple_setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        setup_timeout: int = 300,
        check_interval: int = 10
    ) -> Dict[str, str]:
        """
        ä¸€é”®è®¤è¯è®¾ç½® - è‡ªåŠ¨æ£€æµ‹ç™»å½•çŠ¶æ€å¹¶å…³é—­æµè§ˆå™¨

        Args:
            site_name: ç«™ç‚¹åç§°
            login_url: ç™»å½•é¡µé¢URL
            test_url: æµ‹è¯•é¡µé¢URL
            setup_timeout: æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            check_interval: æ£€æµ‹é—´éš”ï¼ˆç§’ï¼‰

        Returns:
            Dict: è®¾ç½®ç»“æœ
        """
        try:
            # åˆ›å»ºæµè§ˆå™¨é…ç½®
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=30000
            )

            logger.info(f"ğŸš€ å¼€å§‹ä¸€é”®è®¤è¯è®¾ç½®: {site_name}")
            logger.info("ğŸ“– æµè§ˆå™¨å°†ä¼šæ‰“å¼€ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•")
            logger.info("âœ¨ ç™»å½•å®Œæˆåä¼šè‡ªåŠ¨æ£€æµ‹å¹¶å…³é—­æµè§ˆå™¨")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # ç¬¬ä¸€æ­¥ï¼šæ‰“å¼€ç™»å½•é¡µé¢
                logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€ç™»å½•é¡µé¢: {login_url}")
                result = await crawler.arun(url=login_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•æ‰“å¼€ç™»å½•é¡µé¢: {result.error_message}",
                        error_type="setup_failed"
                    )

                logger.info("âœ… ç™»å½•é¡µé¢å·²æ‰“å¼€ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•...")

                # ç¬¬äºŒæ­¥ï¼šå®šæœŸæ£€æµ‹ç™»å½•çŠ¶æ€
                total_wait = 0
                login_detected = False
                last_notification = 0

                while total_wait < setup_timeout and not login_detected:
                    await asyncio.sleep(check_interval)
                    total_wait += check_interval

                    # æ¯60ç§’æé†’ä¸€æ¬¡
                    if total_wait - last_notification >= 60:
                        remaining = setup_timeout - total_wait
                        logger.info(f"â° ç­‰å¾…ç™»å½•ä¸­... å‰©ä½™æ—¶é—´: {remaining} ç§’")
                        last_notification = total_wait

                    try:
                        # æ£€æµ‹ç™»å½•çŠ¶æ€
                        test_result = await crawler.arun(url=test_url, config=config)

                        if test_result.success:
                            # åˆ†æé¡µé¢å†…å®¹åˆ¤æ–­æ˜¯å¦å·²ç™»å½•
                            analysis = self._analyze_login_status(
                                test_result, site_name)

                            if analysis['status'] in ['success', 'likely_logged_in']:
                                login_detected = True
                                logger.info("ğŸ‰ æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼")
                                logger.info("ğŸ”’ æ­£åœ¨ä¿å­˜è®¤è¯é…ç½®...")

                                # å†æ¬¡è®¿é—®æµ‹è¯•é¡µé¢ç¡®ä¿ä¼šè¯ä¿å­˜
                                await crawler.arun(url=test_url, config=config)

                                return {
                                    "status": "success",
                                    "message": "ä¸€é”®è®¤è¯è®¾ç½®å®Œæˆï¼Œç™»å½•çŠ¶æ€å·²ä¿å­˜",
                                    "profile_path": self.get_profile_path(site_name),
                                    "login_detected_at": f"{total_wait}ç§’"
                                }

                    except Exception as e:
                        # æ£€æµ‹è¿‡ç¨‹ä¸­çš„é”™è¯¯ä¸ä¸­æ–­æµç¨‹
                        logger.debug(f"æ£€æµ‹ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
                        continue

                # è¶…æ—¶å¤„ç†
                if not login_detected:
                    logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼Œä½†ä¼šè¯å¯èƒ½å·²ä¿å­˜")
                    return {
                        "status": "timeout",
                        "message": "è®¾ç½®è¶…æ—¶ï¼Œä½†è®¤è¯é…ç½®å¯èƒ½å·²ä¿å­˜ï¼Œè¯·å°è¯•ä½¿ç”¨",
                        "profile_path": self.get_profile_path(site_name)
                    }

            logger.info("ğŸ”š æµè§ˆå™¨å·²è‡ªåŠ¨å…³é—­")

        except Exception as e:
            logger.error(f"ä¸€é”®è®¤è¯è®¾ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"ä¸€é”®è®¤è¯è®¾ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    def _analyze_login_status(self, result, site_name: str) -> Dict[str, str]:
        """åˆ†æé¡µé¢å†…å®¹åˆ¤æ–­ç™»å½•çŠ¶æ€"""
        try:
            content_lower = result.html.lower()

            # é€šç”¨ç™»å½•æˆåŠŸæŒ‡æ ‡
            success_indicators = [
                'logout', 'sign out', 'profile', 'account', 'dashboard',
                'settings', 'my account', 'user menu', 'welcome'
            ]

            # é€šç”¨æœªç™»å½•æŒ‡æ ‡
            login_indicators = [
                'login', 'sign in', 'signin', 'log in', 'authenticate'
            ]

            # ç«™ç‚¹ç‰¹å®šæŒ‡æ ‡
            if site_name == "medium_com":
                success_indicators.extend(
                    ['write', 'stories', 'following', 'notifications'])
                login_indicators.extend(['get started', 'sign up'])
            elif site_name == "investors_com":
                success_indicators.extend(
                    ['premium', 'watchlist', 'portfolio'])
                login_indicators.extend(['subscribe', 'free trial'])

            # è®¡ç®—æŒ‡æ ‡
            success_count = sum(
                1 for indicator in success_indicators if indicator in content_lower)
            login_count = sum(
                1 for indicator in login_indicators if indicator in content_lower)

            # åˆ¤æ–­é€»è¾‘
            if success_count > login_count and success_count >= 2:
                return {
                    "status": "success",
                    "message": f"ç™»å½•æˆåŠŸ (æˆåŠŸæŒ‡æ ‡: {success_count}, ç™»å½•æŒ‡æ ‡: {login_count})",
                    "confidence": "high"
                }
            elif success_count > 0 and login_count == 0:
                return {
                    "status": "likely_logged_in",
                    "message": f"å¯èƒ½å·²ç™»å½• (æˆåŠŸæŒ‡æ ‡: {success_count})",
                    "confidence": "medium"
                }
            elif login_count > success_count:
                return {
                    "status": "not_logged_in",
                    "message": f"å°šæœªç™»å½• (ç™»å½•æŒ‡æ ‡: {login_count})",
                    "confidence": "high"
                }
            else:
                return {
                    "status": "uncertain",
                    "message": "æ— æ³•ç¡®å®šç™»å½•çŠ¶æ€",
                    "confidence": "low"
                }

        except Exception as e:
            return {
                "status": "error",
                "message": f"åˆ†æç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {str(e)}",
                "confidence": "none"
            }

    async def interactive_setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        setup_timeout: int = 600
    ) -> Dict[str, str]:
        """
        äº¤äº’å¼è®¤è¯è®¾ç½® - ç”¨æˆ·æ‰‹åŠ¨ç¡®è®¤ç™»å½•å®Œæˆ

        æµç¨‹ï¼š
        1. æ‰“å¼€æµè§ˆå™¨åˆ°ç™»å½•é¡µé¢
        2. ç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•
        3. ç”¨æˆ·åœ¨æµè§ˆå™¨åœ°å€æ è®¿é—®ç‰¹æ®Šç¡®è®¤URLæ¥ç¡®è®¤ç™»å½•å®Œæˆ
        4. ç³»ç»Ÿæ£€æµ‹åˆ°ç¡®è®¤ä¿¡å·åä¿å­˜é…ç½®å¹¶å…³é—­æµè§ˆå™¨

        Args:
            site_name: ç«™ç‚¹åç§°
            login_url: ç™»å½•é¡µé¢URL
            test_url: æµ‹è¯•é¡µé¢URL
            setup_timeout: æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Dict: è®¾ç½®ç»“æœ
        """
        try:
            # åˆ›å»ºæµè§ˆå™¨é…ç½®
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=30000
            )

            # ç”Ÿæˆç¡®è®¤URL
            confirm_url = f"data:text/html,<html><body><h1>è®¤è¯è®¾ç½®å®Œæˆ!</h1><p>ç«™ç‚¹: {site_name}</p><p>è¯·å…³é—­æ­¤æµè§ˆå™¨çª—å£</p><script>setTimeout(()=>{{window.close();}}, 3000);</script></body></html>"

            logger.info(f"ğŸš€ å¼€å§‹äº¤äº’å¼è®¤è¯è®¾ç½®: {site_name}")
            logger.info("=" * 60)
            logger.info("ğŸ“– æµè§ˆå™¨å³å°†æ‰“å¼€ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
            logger.info("   1. åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•")
            logger.info(f"   2. ç™»å½•æˆåŠŸåï¼Œåœ¨åœ°å€æ è¾“å…¥: about:blank")
            logger.info("   3. ç„¶åè¾“å…¥ä»¥ä¸‹ç¡®è®¤åœ°å€:")
            logger.info(f"      {confirm_url[:100]}...")
            logger.info("   4. æˆ–è€…ç›´æ¥å…³é—­æµè§ˆå™¨çª—å£")
            logger.info("=" * 60)

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # ç¬¬ä¸€æ­¥ï¼šæ‰“å¼€ç™»å½•é¡µé¢
                logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€ç™»å½•é¡µé¢: {login_url}")
                result = await crawler.arun(url=login_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•æ‰“å¼€ç™»å½•é¡µé¢: {result.error_message}",
                        error_type="setup_failed"
                    )

                logger.info("âœ… ç™»å½•é¡µé¢å·²æ‰“å¼€")
                logger.info("ğŸ”‘ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•...")

                # ç­‰å¾…ç”¨æˆ·æ“ä½œ - å®šæœŸæ£€æµ‹æµè§ˆå™¨æ˜¯å¦è¿˜æ´»ç€
                total_wait = 0
                check_interval = 5
                last_notification = 0

                while total_wait < setup_timeout:
                    await asyncio.sleep(check_interval)
                    total_wait += check_interval

                    # æ¯120ç§’æé†’ä¸€æ¬¡
                    if total_wait - last_notification >= 120:
                        remaining = setup_timeout - total_wait
                        logger.info(f"â° ç­‰å¾…ç”¨æˆ·æ“ä½œ... å‰©ä½™æ—¶é—´: {remaining//60} åˆ†é’Ÿ")
                        logger.info("   ç™»å½•å®Œæˆåè¯·å…³é—­æµè§ˆå™¨çª—å£")
                        last_notification = total_wait

                    try:
                        # å°è¯•è®¿é—®æµ‹è¯•é¡µé¢éªŒè¯æµè§ˆå™¨çŠ¶æ€
                        # è¿™é‡Œä¸åšç™»å½•çŠ¶æ€åˆ¤æ–­ï¼Œåªæ˜¯ä¿æŒä¼šè¯æ´»è·ƒ
                        test_result = await crawler.arun(url=test_url, config=config)

                        # æ£€æŸ¥æ˜¯å¦ç”¨æˆ·å°è¯•è®¿é—®ç¡®è®¤é¡µé¢
                        if test_result.success and "è®¤è¯è®¾ç½®å®Œæˆ" in test_result.html:
                            logger.info("ğŸ‰ æ£€æµ‹åˆ°ç”¨æˆ·ç¡®è®¤ä¿¡å·ï¼")
                            break

                    except Exception as e:
                        # å¦‚æœå‡ºç°è¿æ¥é”™è¯¯ï¼Œå¯èƒ½æ˜¯æµè§ˆå™¨è¢«å…³é—­äº†
                        if "disconnected" in str(e).lower() or "closed" in str(e).lower():
                            logger.info("ğŸ”š æ£€æµ‹åˆ°æµè§ˆå™¨å·²å…³é—­")
                            break
                        # å…¶ä»–é”™è¯¯ç»§ç»­ç­‰å¾…
                        continue

                # æœ€ç»ˆéªŒè¯å’Œä¿å­˜
                logger.info("ğŸ’¾ æ­£åœ¨ä¿å­˜è®¤è¯é…ç½®...")

                try:
                    # æœ€åéªŒè¯ä¸€æ¬¡ç™»å½•çŠ¶æ€
                    final_result = await crawler.arun(url=test_url, config=config)
                    if final_result.success:
                        # ç®€å•æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ˜æ˜¾çš„ç™»å½•æç¤º
                        content_lower = final_result.html.lower()
                        obvious_login_signs = ['sign in',
                                               'log in', 'login', 'sign up']

                        login_signs_count = sum(
                            1 for sign in obvious_login_signs if sign in content_lower)

                        if login_signs_count <= 2:  # å®¹å¿å°‘é‡ç™»å½•æç¤ºï¼ˆå¯èƒ½æ˜¯é¡µè„šç­‰ï¼‰
                            status = "success"
                            message = "è®¤è¯é…ç½®è®¾ç½®æˆåŠŸï¼Œç™»å½•çŠ¶æ€å·²ä¿å­˜"
                        else:
                            status = "warning"
                            message = f"è®¤è¯é…ç½®å·²ä¿å­˜ï¼Œä½†æ£€æµ‹åˆ°{login_signs_count}ä¸ªç™»å½•æç¤ºï¼Œè¯·éªŒè¯ç™»å½•çŠ¶æ€"
                    else:
                        status = "warning"
                        message = "è®¤è¯é…ç½®å·²ä¿å­˜ï¼Œä½†æ— æ³•éªŒè¯æœ€ç»ˆçŠ¶æ€"

                except Exception:
                    status = "completed"
                    message = "è®¤è¯é…ç½®å·²ä¿å­˜ï¼Œç”¨æˆ·å·²å…³é—­æµè§ˆå™¨"

                return {
                    "status": status,
                    "message": message,
                    "profile_path": self.get_profile_path(site_name),
                    "duration": f"{total_wait}ç§’"
                }

            logger.info("ğŸ”š æµè§ˆå™¨ä¼šè¯å·²ç»“æŸ")

        except Exception as e:
            logger.error(f"äº¤äº’å¼è®¤è¯è®¾ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"äº¤äº’å¼è®¤è¯è®¾ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    async def quick_setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        wait_time: int = 120
    ) -> Dict[str, str]:
        """
        å¿«é€Ÿè®¤è¯è®¾ç½® - å›ºå®šç­‰å¾…æ—¶é—´ç‰ˆæœ¬

        Args:
            site_name: ç«™ç‚¹åç§°
            login_url: ç™»å½•é¡µé¢URL
            test_url: æµ‹è¯•é¡µé¢URL
            wait_time: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤2åˆ†é’Ÿ

        Returns:
            Dict: è®¾ç½®ç»“æœ
        """
        try:
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=30000
            )

            logger.info(f"ğŸš€ å¿«é€Ÿè®¤è¯è®¾ç½®: {site_name}")
            logger.info(f"â° å°†ç­‰å¾… {wait_time} ç§’ä¾›æ‚¨å®Œæˆç™»å½•")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # æ‰“å¼€ç™»å½•é¡µé¢
                logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€ç™»å½•é¡µé¢: {login_url}")
                result = await crawler.arun(url=login_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•æ‰“å¼€ç™»å½•é¡µé¢: {result.error_message}",
                        error_type="setup_failed"
                    )

                logger.info("âœ… ç™»å½•é¡µé¢å·²æ‰“å¼€ï¼Œè¯·å¼€å§‹ç™»å½•...")

                # åˆ†æ®µç­‰å¾…ï¼Œæ¯30ç§’æé†’ä¸€æ¬¡
                for i in range(0, wait_time, 30):
                    remaining = wait_time - i
                    logger.info(f"â° ç­‰å¾…ç™»å½•ä¸­... å‰©ä½™æ—¶é—´: {remaining} ç§’")
                    await asyncio.sleep(min(30, remaining))

                logger.info("â° ç­‰å¾…æ—¶é—´ç»“æŸï¼Œæ­£åœ¨ä¿å­˜é…ç½®...")

                # è®¿é—®æµ‹è¯•é¡µé¢ä¿å­˜ä¼šè¯
                try:
                    await crawler.arun(url=test_url, config=config)
                    logger.info("ğŸ’¾ ä¼šè¯å·²ä¿å­˜")
                except Exception:
                    logger.warning("âš ï¸ ä¿å­˜ä¼šè¯æ—¶å‡ºç°è­¦å‘Šï¼Œä½†é…ç½®å¯èƒ½ä»æœ‰æ•ˆ")

                return {
                    "status": "completed",
                    "message": f"å¿«é€Ÿè®¤è¯è®¾ç½®å®Œæˆï¼Œå·²ç­‰å¾…{wait_time}ç§’",
                    "profile_path": self.get_profile_path(site_name)
                }

            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")

        except Exception as e:
            logger.error(f"å¿«é€Ÿè®¤è¯è®¾ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"å¿«é€Ÿè®¤è¯è®¾ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    async def quick_setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        wait_time: int = 120
    ) -> Dict[str, str]:
        """
        å¿«é€Ÿè®¤è¯è®¾ç½® - ä¿®å¤æµè§ˆå™¨è‡ªåŠ¨å…³é—­é—®é¢˜

        Args:
            site_name: ç«™ç‚¹åç§°
            login_url: ç™»å½•é¡µé¢URL
            test_url: æµ‹è¯•é¡µé¢URL
            wait_time: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤2åˆ†é’Ÿ

        Returns:
            Dict: è®¾ç½®ç»“æœ
        """
        try:
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=60000  # å¢åŠ é¡µé¢è¶…æ—¶æ—¶é—´
            )

            logger.info(f"ğŸš€ å¿«é€Ÿè®¤è¯è®¾ç½®: {site_name}")
            logger.info(f"â° å°†ç­‰å¾… {wait_time} ç§’ä¾›æ‚¨å®Œæˆç™»å½•")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # æ‰“å¼€ç™»å½•é¡µé¢
                logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€ç™»å½•é¡µé¢: {login_url}")
                result = await crawler.arun(url=login_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•æ‰“å¼€ç™»å½•é¡µé¢: {result.error_message}",
                        error_type="setup_failed"
                    )

                logger.info("âœ… ç™»å½•é¡µé¢å·²æ‰“å¼€ï¼Œè¯·å¼€å§‹ç™»å½•...")

                # ä¿®å¤ï¼šåœ¨ç­‰å¾…æœŸé—´ä¿æŒé¡µé¢æ´»è·ƒï¼Œé˜²æ­¢è¢«è‡ªåŠ¨å…³é—­
                elapsed_time = 0
                check_interval = 30  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡

                while elapsed_time < wait_time:
                    remaining = wait_time - elapsed_time
                    logger.info(f"â° ç­‰å¾…ç™»å½•ä¸­... å‰©ä½™æ—¶é—´: {remaining} ç§’")

                    # ç­‰å¾…æ—¶é—´ï¼ˆä½†ä¸è¶…è¿‡å‰©ä½™æ—¶é—´ï¼‰
                    sleep_time = min(check_interval, remaining)
                    await asyncio.sleep(sleep_time)
                    elapsed_time += sleep_time

                    # å…³é”®ä¿®å¤ï¼šå®šæœŸè®¿é—®é¡µé¢ä¿æŒæ´»è·ƒï¼Œé˜²æ­¢è¢«å…³é—­
                    try:
                        # ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´è¿›è¡Œè½»é‡çº§æ£€æŸ¥
                        keep_alive_config = CrawlerRunConfig(
                            cache_mode=CacheMode.BYPASS,
                            page_timeout=10000  # 10ç§’è¶…æ—¶
                        )

                        # è®¿é—®å½“å‰é¡µé¢ä¿æŒæ´»è·ƒï¼ˆä¸è¾“å‡ºç»“æœï¼‰
                        await crawler.arun(url=login_url, config=keep_alive_config)
                        logger.debug("ğŸ”„ é¡µé¢ä¿æ´»æ£€æŸ¥å®Œæˆ")

                    except Exception as e:
                        # å¦‚æœä¿æ´»å¤±è´¥ï¼Œè®°å½•ä½†ç»§ç»­ç­‰å¾…
                        logger.debug(f"ä¿æ´»æ£€æŸ¥å‡ºç°å¼‚å¸¸: {str(e)}")
                        continue

                logger.info("â° ç­‰å¾…æ—¶é—´ç»“æŸï¼Œæ­£åœ¨ä¿å­˜è®¤è¯é…ç½®...")

                # è®¿é—®æµ‹è¯•é¡µé¢ä¿å­˜ä¼šè¯
                try:
                    final_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=30000
                    )
                    test_result = await crawler.arun(url=test_url, config=final_config)

                    if test_result.success:
                        logger.info("ğŸ’¾ è®¤è¯ä¼šè¯å·²ä¿å­˜")

                        # ç®€å•éªŒè¯ç™»å½•çŠ¶æ€
                        content_lower = test_result.html.lower()
                        login_indicators = ['sign in',
                                            'log in', 'login', 'sign up']
                        login_count = sum(
                            1 for indicator in login_indicators if indicator in content_lower)

                        if login_count <= 2:
                            status = "success"
                            message = f"è®¤è¯è®¾ç½®æˆåŠŸï¼Œç­‰å¾…æ—¶é—´{wait_time}ç§’"
                        else:
                            status = "warning"
                            message = f"è®¤è¯é…ç½®å·²ä¿å­˜ï¼Œä½†æ£€æµ‹åˆ°{login_count}ä¸ªç™»å½•æç¤ºï¼Œå»ºè®®éªŒè¯"
                    else:
                        status = "completed"
                        message = f"è®¤è¯è®¾ç½®å®Œæˆï¼Œç­‰å¾…æ—¶é—´{wait_time}ç§’ï¼Œè¯·æµ‹è¯•ä½¿ç”¨"

                except Exception as e:
                    logger.warning(f"ä¿å­˜ä¼šè¯æ—¶å‡ºç°è­¦å‘Š: {str(e)}")
                    status = "completed"
                    message = f"è®¤è¯è®¾ç½®å®Œæˆï¼Œç­‰å¾…æ—¶é—´{wait_time}ç§’ï¼Œé…ç½®å¯èƒ½å·²ä¿å­˜"

                return {
                    "status": status,
                    "message": message,
                    "profile_path": self.get_profile_path(site_name)
                }

            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")

        except Exception as e:
            logger.error(f"å¿«é€Ÿè®¤è¯è®¾ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"å¿«é€Ÿè®¤è¯è®¾ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    async def simple_wait_setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        wait_time: int = 120
    ) -> Dict[str, str]:
        """
        ç®€å•ç­‰å¾…ç‰ˆè®¤è¯è®¾ç½® - æœ€å°åŒ–å¹²é¢„

        ç­–ç•¥ï¼šæ‰“å¼€é¡µé¢åçº¯ç­‰å¾…ï¼Œæœ€åä¿å­˜ï¼Œé¿å…ä¸­é—´æ“ä½œå¯¼è‡´é¡µé¢å…³é—­
        """
        try:
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False
            )

            # è®¾ç½®è¾ƒé•¿çš„é¡µé¢è¶…æ—¶æ—¶é—´
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=max(wait_time * 1000 + 30000,
                                 60000)  # ç­‰å¾…æ—¶é—´+30ç§’çš„ç¼“å†²
            )

            logger.info(f"ğŸš€ ç®€å•ç­‰å¾…ç‰ˆè®¤è¯è®¾ç½®: {site_name}")
            logger.info(f"â° æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€ {wait_time} ç§’")
            logger.info("ğŸ”‘ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ŒæœŸé—´è¯·å‹¿å…³é—­æµè§ˆå™¨")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # ç¬¬ä¸€æ­¥ï¼šæ‰“å¼€ç™»å½•é¡µé¢
                logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€ç™»å½•é¡µé¢: {login_url}")
                result = await crawler.arun(url=login_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•æ‰“å¼€ç™»å½•é¡µé¢: {result.error_message}",
                        error_type="setup_failed"
                    )

                logger.info("âœ… ç™»å½•é¡µé¢å·²æ‰“å¼€")
                logger.info("ğŸ’¡ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•...")

                # ç¬¬äºŒæ­¥ï¼šçº¯ç­‰å¾…ï¼Œé¿å…ä»»ä½•å¯èƒ½è§¦å‘é¡µé¢å…³é—­çš„æ“ä½œ
                for i in range(wait_time // 30):
                    remaining = wait_time - (i * 30)
                    logger.info(f"â° ç­‰å¾…ä¸­... å‰©ä½™çº¦ {remaining} ç§’")
                    await asyncio.sleep(30)

                # å¤„ç†å‰©ä½™çš„ä¸è¶³30ç§’çš„æ—¶é—´
                final_wait = wait_time % 30
                if final_wait > 0:
                    logger.info(f"â° æœ€åç­‰å¾… {final_wait} ç§’...")
                    await asyncio.sleep(final_wait)

                logger.info("â° ç­‰å¾…å®Œæˆï¼Œæ­£åœ¨ä¿å­˜è®¤è¯çŠ¶æ€...")

                # ç¬¬ä¸‰æ­¥ï¼šè®¿é—®æµ‹è¯•é¡µé¢ä»¥ä¿å­˜è®¤è¯çŠ¶æ€
                try:
                    save_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=30000
                    )

                    save_result = await crawler.arun(url=test_url, config=save_config)

                    if save_result.success:
                        logger.info("ğŸ’¾ è®¤è¯çŠ¶æ€å·²ä¿å­˜")
                        return {
                            "status": "completed",
                            "message": f"ç®€å•ç­‰å¾…ç‰ˆè®¾ç½®å®Œæˆï¼Œå·²ç­‰å¾…{wait_time}ç§’",
                            "profile_path": self.get_profile_path(site_name)
                        }
                    else:
                        logger.warning("ä¿å­˜è®¤è¯çŠ¶æ€æ—¶å‡ºç°é—®é¢˜ï¼Œä½†é…ç½®æ–‡ä»¶å¯èƒ½å·²ç”Ÿæˆ")
                        return {
                            "status": "warning",
                            "message": f"è®¾ç½®å®Œæˆä½†ä¿å­˜çŠ¶æ€å¼‚å¸¸ï¼Œå·²ç­‰å¾…{wait_time}ç§’ï¼Œè¯·æµ‹è¯•ä½¿ç”¨",
                            "profile_path": self.get_profile_path(site_name)
                        }

                except Exception as e:
                    logger.warning(f"ä¿å­˜é˜¶æ®µå‡ºç°å¼‚å¸¸: {str(e)}")
                    return {
                        "status": "completed",
                        "message": f"è®¾ç½®å®Œæˆï¼Œå·²ç­‰å¾…{wait_time}ç§’ï¼Œé…ç½®å¯èƒ½å·²ä¿å­˜",
                        "profile_path": self.get_profile_path(site_name)
                    }

            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")

        except Exception as e:
            logger.error(f"ç®€å•ç­‰å¾…ç‰ˆè®¤è¯è®¾ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"ç®€å•ç­‰å¾…ç‰ˆè®¤è¯è®¾ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )


# åˆ›å»ºæœåŠ¡å®ä¾‹
auth_crawler_service = AuthCrawlerService()

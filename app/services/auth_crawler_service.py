# app/services/auth_crawler_service.py
import asyncio
import logging
import os
import glob
from pathlib import Path
from typing import Dict, Optional

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from app.models.models import CrawlRequest, CrawlData, MarkdownRequest, MarkdownData
from app.services.crawler_service import CrawlerException

logger = logging.getLogger(__name__)


class AuthCrawlerService:
    """å¸¦è®¤è¯åŠŸèƒ½çš„çˆ¬è™«æœåŠ¡"""

    def __init__(self):
        # è®¤è¯é…ç½®æ–‡ä»¶å­˜å‚¨ç›®å½•
        self.auth_profiles_dir = Path("./auth_profiles")
        self.auth_profiles_dir.mkdir(parents=True, exist_ok=True)

    def get_profile_path(self, site_name: str) -> str:
        """è·å–æŒ‡å®šç«™ç‚¹çš„è®¤è¯é…ç½®æ–‡ä»¶è·¯å¾„"""
        return str((self.auth_profiles_dir / site_name).resolve())

    def _get_browser_executable_path(self) -> Optional[str]:
        """è·å–æµè§ˆå™¨å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„"""

        # 1. ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
        env_path = os.environ.get('CHROMIUM_EXECUTABLE_PATH')
        if env_path and os.path.exists(env_path):
            logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æµè§ˆå™¨: {env_path}")
            return env_path

        # 2. ä»é…ç½®æ–‡ä»¶è¯»å–
        config_file = Path("./browser_config.txt")
        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    path = f.read().strip()
                    if path and os.path.exists(path):
                        logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶æŒ‡å®šçš„æµè§ˆå™¨: {path}")
                        return path
            except Exception:
                pass

        # 3. è‡ªåŠ¨æ£€æµ‹
        auto_path = self._auto_detect_chromium()
        if auto_path:
            logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°æµè§ˆå™¨: {auto_path}")
            return auto_path

        # 4. å°è¯•ç”¨æˆ·æŠ¥å‘Šçš„å…·ä½“è·¯å¾„
        user_reported_path = "/Users/M16/Library/Caches/ms-playwright/chromium-1169/chrome-mac/Chromium.app/Contents/MacOS/Chromium"
        if os.path.exists(user_reported_path):
            logger.info(f"ä½¿ç”¨æ£€æµ‹åˆ°çš„æµè§ˆå™¨è·¯å¾„: {user_reported_path}")
            return user_reported_path

        logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„æµè§ˆå™¨è·¯å¾„")
        return None

    def _auto_detect_chromium(self) -> Optional[str]:
        """è‡ªåŠ¨æ£€æµ‹ Chromium è·¯å¾„"""
        import platform

        system = platform.system()

        if system == "Darwin":  # macOS
            patterns = [
                "/Users/*/Library/Caches/ms-playwright/chromium-*/chrome-mac/Chromium.app/Contents/MacOS/Chromium",
                "/Applications/Chromium.app/Contents/MacOS/Chromium",
                "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
            ]
        elif system == "Linux":
            patterns = [
                "/home/*/snap/chromium/*/usr/lib/chromium-browser/chrome",
                "/usr/bin/chromium-browser",
                "/usr/bin/google-chrome"
            ]
        elif system == "Windows":
            patterns = [
                "C:/Users/*/AppData/Local/ms-playwright/chromium-*/chrome-win/chrome.exe",
                "C:/Program Files/Google/Chrome/Application/chrome.exe"
            ]
        else:
            return None

        for pattern in patterns:
            matches = glob.glob(pattern)
            if matches:
                # é€‰æ‹©ç‰ˆæœ¬å·æœ€é«˜çš„
                latest = sorted(matches, reverse=True)[0]
                if os.path.exists(latest):
                    return latest

        return None

    def _create_auth_browser_config(
        self,
        site_name: str,
        js_enabled: bool = True,
        headless: bool = True
    ) -> BrowserConfig:
        """åˆ›å»ºå¸¦è®¤è¯çš„æµè§ˆå™¨é…ç½®"""
        user_data_dir = self.get_profile_path(site_name)

        # å¼ºåˆ¶è·å–æµè§ˆå™¨è·¯å¾„
        browser_path = self._get_browser_executable_path()

        # åˆ›å»ºæµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=headless,
            java_script_enabled=js_enabled,
            use_persistent_context=True,
            user_data_dir=user_data_dir,
            viewport={"width": 1280, "height": 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            verbose=True
        )

        # å¼ºåˆ¶è®¾ç½®æµè§ˆå™¨è·¯å¾„
        if browser_path:
            browser_config.browser_executable_path = browser_path
            logger.info(f"âœ… å¼ºåˆ¶è®¾ç½®æµè§ˆå™¨è·¯å¾„: {browser_path}")
        else:
            # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å·²çŸ¥è·¯å¾„
            fallback_path = "/Users/M16/Library/Caches/ms-playwright/chromium-1169/chrome-mac/Chromium.app/Contents/MacOS/Chromium"
            if os.path.exists(fallback_path):
                browser_config.browser_executable_path = fallback_path
                logger.info(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨è·¯å¾„: {fallback_path}")
            else:
                logger.error("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æµè§ˆå™¨è·¯å¾„")
                raise CrawlerException(
                    message=f"æœªæ‰¾åˆ°å¯ç”¨çš„ Chromium æµè§ˆå™¨ã€‚è¯·ç¡®ä¿å·²å®‰è£… Playwright æµè§ˆå™¨æˆ–è®¾ç½®æ­£ç¡®çš„æµè§ˆå™¨è·¯å¾„",
                    error_type="browser_not_found"
                )

        # éªŒè¯è·¯å¾„æ˜¯å¦å¯æ‰§è¡Œ
        if hasattr(browser_config, 'browser_executable_path') and browser_config.browser_executable_path:
            if not os.access(browser_config.browser_executable_path, os.X_OK):
                logger.warning(
                    f"âš ï¸ æµè§ˆå™¨æ–‡ä»¶ä¸å¯æ‰§è¡Œï¼Œå°è¯•ä¿®å¤æƒé™: {browser_config.browser_executable_path}")
                try:
                    os.chmod(browser_config.browser_executable_path, 0o755)
                    logger.info("âœ… æƒé™ä¿®å¤æˆåŠŸ")
                except Exception as e:
                    logger.error(f"âŒ æƒé™ä¿®å¤å¤±è´¥: {e}")

        return browser_config

    async def setup_auth_profile(
        self,
        site_name: str,
        login_url: str,
        test_url: str,
        setup_timeout: int = 300
    ) -> Dict[str, str]:
        """
        è®¾ç½®è®¤è¯é…ç½®æ–‡ä»¶ - æ‰“å¼€å¯è§æµè§ˆå™¨ä¾›æ‰‹åŠ¨ç™»å½•

        Args:
            site_name: ç«™ç‚¹åç§°ï¼Œç”¨ä½œé…ç½®æ–‡ä»¶å
            login_url: ç™»å½•é¡µé¢URL
            test_url: ç”¨äºæµ‹è¯•ç™»å½•çŠ¶æ€çš„URL
            setup_timeout: è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Dict: åŒ…å«æ“ä½œç»“æœçš„å­—å…¸
        """
        try:
            # åˆ›å»ºå¯è§æµè§ˆå™¨é…ç½®ï¼ˆç”¨äºæ‰‹åŠ¨ç™»å½•ï¼‰
            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                headless=False  # å¯è§æ¨¡å¼
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=setup_timeout * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            )

            logger.info(f"æ­£åœ¨ä¸º {site_name} è®¾ç½®è®¤è¯é…ç½®æ–‡ä»¶...")
            logger.info(f"å°†æ‰“å¼€æµè§ˆå™¨çª—å£ï¼Œè¯·æ‰‹åŠ¨å®Œæˆç™»å½•è¿‡ç¨‹")

            async with AsyncWebCrawler(config=browser_config) as crawler:
                # å…ˆè®¿é—®ç™»å½•é¡µé¢
                logger.info(f"è®¿é—®ç™»å½•é¡µé¢: {login_url}")
                login_result = await crawler.arun(url=login_url, config=config)

                if not login_result.success:
                    raise CrawlerException(
                        message=f"æ— æ³•è®¿é—®ç™»å½•é¡µé¢: {login_result.error_message}",
                        error_type="setup_failed"
                    )

                # è®¿é—®æµ‹è¯•é¡µé¢ä»¥éªŒè¯ç™»å½•çŠ¶æ€å¹¶ä¿å­˜ä¼šè¯
                logger.info(f"è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åè„šæœ¬å°†éªŒè¯ç™»å½•çŠ¶æ€...")
                logger.info(f"éªŒè¯ç™»å½•çŠ¶æ€: {test_url}")

                test_result = await crawler.arun(url=test_url, config=config)

                if test_result.success:
                    # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸçš„ç®€å•éªŒè¯
                    content_lower = test_result.html.lower()
                    if any(keyword in content_lower for keyword in ['login', 'sign in', 'signin']) and \
                       not any(keyword in content_lower for keyword in ['logout', 'sign out', 'account', 'profile']):
                        logger.warning("å¯èƒ½ä»æœªç™»å½•ï¼Œè¯·æ£€æŸ¥ç™»å½•çŠ¶æ€")
                        return {
                            "status": "warning",
                            "message": "è®¤è¯é…ç½®å·²ä¿å­˜ï¼Œä½†å¯èƒ½æœªæˆåŠŸç™»å½•ï¼Œè¯·æ£€æŸ¥",
                            "profile_path": self.get_profile_path(site_name)
                        }
                    else:
                        logger.info("è®¤è¯é…ç½®è®¾ç½®æˆåŠŸï¼")
                        return {
                            "status": "success",
                            "message": "è®¤è¯é…ç½®è®¾ç½®æˆåŠŸï¼Œä¼šè¯å·²ä¿å­˜",
                            "profile_path": self.get_profile_path(site_name)
                        }
                else:
                    raise CrawlerException(
                        message=f"éªŒè¯ç™»å½•çŠ¶æ€å¤±è´¥: {test_result.error_message}",
                        error_type="setup_failed"
                    )

        except Exception as e:
            logger.error(f"è®¾ç½®è®¤è¯é…ç½®å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"è®¾ç½®è®¤è¯é…ç½®å¤±è´¥: {str(e)}",
                error_type="setup_failed"
            )

    async def crawl_with_auth(
        self,
        site_name: str,
        request: CrawlRequest
    ) -> CrawlData:
        """
        ä½¿ç”¨ä¿å­˜çš„è®¤è¯é…ç½®çˆ¬å–URL

        Args:
            site_name: ç«™ç‚¹åç§°
            request: çˆ¬å–è¯·æ±‚

        Returns:
            CrawlData: çˆ¬å–ç»“æœ
        """
        try:
            # æ£€æŸ¥è®¤è¯é…ç½®æ˜¯å¦å­˜åœ¨
            profile_path = self.get_profile_path(site_name)
            if not os.path.exists(profile_path):
                raise CrawlerException(
                    message=f"è®¤è¯é…ç½®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè°ƒç”¨ setup_auth_profile è®¾ç½®è®¤è¯",
                    error_type="auth_required"
                )

            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                js_enabled=request.js_enabled,
                headless=True  # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æ— å¤´æ¨¡å¼
            )

            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
                page_timeout=60000,
                wait_for_images=request.include_images,
            )

            if request.css_selector:
                config.css_selector = request.css_selector

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=config)

                if not result.success:
                    # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°ç™»å½•é¡µé¢
                    if result.status_code in [401, 403] or \
                       any(keyword in result.html.lower() for keyword in ['login', 'sign in', 'signin']):
                        raise CrawlerException(
                            message="è®¤è¯å·²è¿‡æœŸï¼Œè¯·é‡æ–°è®¾ç½®è®¤è¯é…ç½®",
                            error_type="auth_expired"
                        )
                    else:
                        raise CrawlerException(
                            message=getattr(result, 'error_message', 'çˆ¬å–å¤±è´¥'),
                            error_type="crawl_failed"
                        )

                return CrawlData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    markdown=result.markdown,
                    media=result.media if hasattr(result, 'media') else None,
                    links=result.links if hasattr(result, 'links') else None
                )

        except CrawlerException:
            raise
        except Exception as e:
            logger.error(f"è®¤è¯çˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"è®¤è¯çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    async def crawl_markdown_with_auth(
        self,
        site_name: str,
        request: MarkdownRequest
    ) -> MarkdownData:
        """
        ä½¿ç”¨ä¿å­˜çš„è®¤è¯é…ç½®è·å–Markdown

        Args:
            site_name: ç«™ç‚¹åç§°
            request: Markdownè¯·æ±‚

        Returns:
            MarkdownData: Markdownæ•°æ®
        """
        try:
            # æ£€æŸ¥è®¤è¯é…ç½®æ˜¯å¦å­˜åœ¨
            profile_path = self.get_profile_path(site_name)
            if not os.path.exists(profile_path):
                raise CrawlerException(
                    message=f"è®¤è¯é…ç½®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®¾ç½®è®¤è¯",
                    error_type="auth_required"
                )

            browser_config = self._create_auth_browser_config(
                site_name=site_name,
                js_enabled=request.js_enabled,
                headless=True
            )

            # åˆ›å»ºMarkdownä¸“ç”¨é…ç½®ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥å‚è€ƒåŸæœåŠ¡çš„markdowné…ç½®ï¼‰
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
                page_timeout=60000,
            )

            if request.css_selector:
                config.css_selector = request.css_selector

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=request.url, config=config)

                if not result.success:
                    if result.status_code in [401, 403] or \
                       any(keyword in result.html.lower() for keyword in ['login', 'sign in']):
                        raise CrawlerException(
                            message="è®¤è¯å·²è¿‡æœŸï¼Œè¯·é‡æ–°è®¾ç½®è®¤è¯é…ç½®",
                            error_type="auth_expired"
                        )
                    else:
                        raise CrawlerException(
                            message=getattr(
                                result, 'error_message', 'Markdownè·å–å¤±è´¥'),
                            error_type="crawl_failed"
                        )

                # è§£æç»“æœ
                title = None
                if hasattr(result, 'metadata') and result.metadata:
                    title = result.metadata.get('title')

                raw_markdown = result.markdown if hasattr(
                    result, 'markdown') else None
                word_count = len(raw_markdown.split()) if raw_markdown else 0

                return MarkdownData(
                    url=request.url,
                    status_code=getattr(result, 'status_code', None),
                    raw_markdown=raw_markdown,
                    fit_markdown=raw_markdown,  # ç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ·»åŠ è¿‡æ»¤é€»è¾‘
                    title=title,
                    word_count=word_count
                )

        except CrawlerException:
            raise
        except Exception as e:
            logger.error(f"è®¤è¯Markdownçˆ¬å–å¤±è´¥ {request.url}: {str(e)}")
            raise CrawlerException(
                message=f"è®¤è¯Markdownè·å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                error_type="unexpected"
            )

    def list_auth_profiles(self) -> Dict[str, Dict]:
        """åˆ—å‡ºæ‰€æœ‰å·²è®¾ç½®çš„è®¤è¯é…ç½®"""
        profiles = {}

        for profile_dir in self.auth_profiles_dir.iterdir():
            if profile_dir.is_dir():
                profiles[profile_dir.name] = {
                    "site_name": profile_dir.name,
                    "profile_path": str(profile_dir.resolve()),
                    "created_time": profile_dir.stat().st_mtime
                }

        return profiles

    def delete_auth_profile(self, site_name: str) -> bool:
        """åˆ é™¤æŒ‡å®šçš„è®¤è¯é…ç½®"""
        import shutil

        profile_path = Path(self.get_profile_path(site_name))
        if profile_path.exists():
            shutil.rmtree(profile_path)
            logger.info(f"å·²åˆ é™¤è®¤è¯é…ç½®: {site_name}")
            return True
        return False


# åˆ›å»ºæœåŠ¡å®ä¾‹
auth_crawler_service = AuthCrawlerService()

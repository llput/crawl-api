# app/routers/crawls.py

# æ ‡å‡†åº“å¯¼å…¥
import os
import asyncio
from pathlib import Path

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from fastapi import APIRouter
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

# æœ¬åœ°åº”ç”¨å¯¼å…¥
from app.models.models import CrawlRequest, MarkdownRequest, ScreenshotRequest, CrawlData, MarkdownData, ScreenshotData
from app.models.response import (
    ApiResponse, BusinessCode,
    CrawlResponse, MarkdownResponse, HealthResponse, ScreenshotResponse
)
from app.services.crawler_service import crawler_service, CrawlerException
from app.utils.helpers import is_valid_url

router = APIRouter(
    prefix="/api/v1/crawl",
    tags=["çˆ¬å–"]
)


@router.post("/url", response_model=CrawlResponse)
async def crawl_single_url(request: CrawlRequest) -> CrawlResponse:
    """
    çˆ¬å–å•ä¸ªURLå¹¶è¿”å›å®Œæ•´ç»“æœ
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        data = await crawler_service.crawl_url(request)
        return ApiResponse.success_response(
            data=data,
            message="çˆ¬å–æˆåŠŸ"
        )

    except CrawlerException as e:
        # æ ¹æ®å¼‚å¸¸ç±»å‹æ˜ å°„é”™è¯¯ç 
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "crawl_failed": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.post("/markdown", response_model=MarkdownResponse)
async def crawl_markdown(request: MarkdownRequest) -> MarkdownResponse:
    """
    ä¸“é—¨è·å–é¡µé¢çš„Markdownå†…å®¹

    æ”¯æŒå¤šç§æ ¼å¼:
    - raw: åŸå§‹markdownå†…å®¹
    - fit: ç»è¿‡å†…å®¹è¿‡æ»¤çš„markdown,æ›´é€‚åˆAIå¤„ç†
    - both: åŒæ—¶è¿”å›ä¸¤ç§æ ¼å¼
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        data = await crawler_service.crawl_markdown(request)
        return ApiResponse.success_response(
            data=data,
            message="Markdownè·å–æˆåŠŸ"
        )

    except CrawlerException as e:
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "crawl_failed": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.post("/screenshot", response_model=ScreenshotResponse)
async def take_screenshot(request: ScreenshotRequest) -> ScreenshotResponse:
    """
    æˆªå–é¡µé¢æˆªå›¾

    æ”¯æŒåŠŸèƒ½:
    - å…¨é¡µé¢æˆªå›¾æˆ–æŒ‡å®šCSSé€‰æ‹©å™¨åŒºåŸŸæˆªå›¾
    - è‡ªå®šä¹‰è§†çª—å¤§å°
    - JavaScriptæ‰§è¡Œæ§åˆ¶
    - ç¼“å­˜æ§åˆ¶
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        data = await crawler_service.take_screenshot(request)
        return ApiResponse.success_response(
            data=data,
            message="æˆªå›¾æˆåŠŸ"
        )

    except CrawlerException as e:
        # æ ¹æ®å¼‚å¸¸ç±»å‹æ˜ å°„é”™è¯¯ç 
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "screenshot_failed": BusinessCode.CRAWL_FAILED,
            "screenshot_empty": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.get("/health", response_model=HealthResponse)
async def health_check() -> HealthResponse:
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    return ApiResponse.success_response(
        data={"status": "healthy", "service": "crawl4ai-api"},
        message="æœåŠ¡æ­£å¸¸"
    )


@router.post("/debug/extension", response_model=ApiResponse[dict])
async def debug_extension_loading(request: CrawlRequest) -> ApiResponse[dict]:
    """
    è°ƒè¯•æ¥å£ï¼šéªŒè¯æ‰©å±•æ˜¯å¦æˆåŠŸåŠ è½½

    è¿™ä¸ªæ¥å£ä¼šä¿æŒæµè§ˆå™¨æ‰“å¼€æ›´é•¿æ—¶é—´ï¼Œæ–¹ä¾¿æ‰‹åŠ¨éªŒè¯æ‰©å±•æ˜¯å¦å·¥ä½œ
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        # æ£€æŸ¥æ‰©å±•è·¯å¾„
        extension_path = None
        env_path = os.environ.get('CHROME_EXTENSION_PATH')
        if env_path and os.path.exists(env_path):
            extension_path = env_path
        else:
            project_extension_path = Path(
                "./chrome-extension/bypass-paywalls-chrome-clean")
            if project_extension_path.exists():
                extension_path = str(project_extension_path.resolve())

        debug_info = {
            "extension_detected": extension_path is not None,
            "extension_path": extension_path,
            "url_tested": request.url,
            "debug_mode": True,
            "browser_will_stay_open": True
        }

        if not extension_path:
            return ApiResponse.success_response(
                data=debug_info,
                message="âš ï¸ æœªæ£€æµ‹åˆ°æ‰©å±•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„"
            )

        # å¼ºåˆ¶ä½¿ç”¨è°ƒè¯•æ¨¡å¼é…ç½®
        browser_config = BrowserConfig(
            headless=False,  # å¼ºåˆ¶å¯è§æ¨¡å¼
            java_script_enabled=request.js_enabled,
            viewport={"width": 1280, "height": 800},
            verbose=True,
            extra_args=[
                f"--load-extension={extension_path}",
                f"--disable-extensions-except={extension_path}",
                "--disable-extensions-except-devtools",
                "--enable-extensions",  # ç¡®ä¿æ‰©å±•å¯ç”¨
            ]
        )

        config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            page_timeout=120000,  # 2åˆ†é’Ÿè¶…æ—¶
            wait_for_images=request.include_images,
        )

        if request.css_selector:
            config.css_selector = request.css_selector

        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=request.url, config=config)

            # é¢å¤–ç­‰å¾…æ—¶é—´ï¼Œè®©ç”¨æˆ·æ£€æŸ¥æµè§ˆå™¨
            print("ğŸ” æµè§ˆå™¨å°†ä¿æŒæ‰“å¼€30ç§’ï¼Œè¯·æ£€æŸ¥æ‰©å±•æ˜¯å¦å·²åŠ è½½...")
            print("   åœ¨æµè§ˆå™¨åœ°å€æ è¾“å…¥ chrome://extensions/ æŸ¥çœ‹æ‰©å±•çŠ¶æ€")
            await asyncio.sleep(30)  # ä¿æŒ30ç§’

            debug_info.update({
                "crawl_success": result.success,
                "status_code": getattr(result, 'status_code', None),
                "content_length": len(result.markdown) if result.markdown else 0,
                "page_title_detected": "subscribe" not in result.markdown.lower() if result.markdown else False,
                "paywall_bypassed": "paywall" not in result.markdown.lower() if result.markdown else False
            })

            if not result.success:
                debug_info["error_message"] = getattr(
                    result, 'error_message', 'æœªçŸ¥é”™è¯¯')

        return ApiResponse.success_response(
            data=debug_info,
            message="ğŸ” æ‰©å±•è°ƒè¯•å®Œæˆï¼Œè¯·æŸ¥çœ‹æµè§ˆå™¨æ˜¯å¦æ˜¾ç¤ºäº†æ‰©å±•å›¾æ ‡"
        )

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"è°ƒè¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        )

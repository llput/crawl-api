# app/routers/crawls.py

# æ ‡å‡†åº“å¯¼å…¥
import os
import asyncio
from pathlib import Path

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from fastapi import APIRouter, Query
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

# æœ¬åœ°åº”ç”¨å¯¼å…¥
from app.models.models import CrawlRequest, MarkdownRequest, ScreenshotRequest, CrawlData, MarkdownData, ScreenshotData, MarkdownFormat
from app.models.response import (
    ApiResponse, BusinessCode,
    CrawlResponse, MarkdownResponse, HealthResponse, ScreenshotResponse
)
from app.services.crawler_service import crawler_service, CrawlerException
from app.utils.helpers import is_valid_url

router = APIRouter(
    prefix="/api/v1/crawl",
    tags=["çˆ¬å–"]
)


# è¾…åŠ©å‡½æ•°


async def _crawl_markdown_with_clean_config(request: MarkdownRequest) -> MarkdownData:
    """ä½¿ç”¨è¶…çº§æ¸…ç†é…ç½®çš„å†…éƒ¨å‡½æ•°"""
    from crawl4ai.content_filter_strategy import PruningContentFilter
    from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

    # æœ€æ¿€è¿›çš„æ¸…ç†é€‰é¡¹
    super_clean_options = {
        "ignore_links": True,
        "skip_internal_links": True,
        "escape_html": False,
        "body_width": 0,
        "unicode_snob": True,
        "default_image_alt": "[å›¾ç‰‡]",
        "mark_code": True,
        "handle_code_in_pre": True,
        "include_sup_sub": False,
    }

    # æ›´æ¿€è¿›çš„å†…å®¹è¿‡æ»¤å™¨
    content_filter = PruningContentFilter(
        threshold=0.2,  # éå¸¸ä½çš„é˜ˆå€¼ï¼Œæ¿€è¿›è¿‡æ»¤
        threshold_type="dynamic",
        min_word_threshold=15,  # æ®µè½è‡³å°‘15ä¸ªè¯
    )

    md_generator = DefaultMarkdownGenerator(
        content_filter=content_filter,
        options=super_clean_options
    )

    browser_config = crawler_service._create_browser_config(request.js_enabled)
    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
        page_timeout=90000,
        markdown_generator=md_generator,
        wait_for_images=True,
    )

    if request.css_selector:
        config.css_selector = request.css_selector

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=request.url, config=config)

        if not result.success:
            raise CrawlerException(
                message=getattr(result, 'error_message', 'è¶…çº§æ¸…ç†æ¨¡å¼è·å–å¤±è´¥'),
                error_type="crawl_failed"
            )

        # è§£æç»“æœ
        title = None
        if hasattr(result, 'metadata') and result.metadata:
            title = result.metadata.get('title')

        # è·å–æ¸…ç†åçš„å†…å®¹
        clean_markdown = None
        if hasattr(result, 'markdown'):
            if hasattr(result.markdown, 'fit_markdown'):
                clean_markdown = result.markdown.fit_markdown
            elif isinstance(result.markdown, str):
                clean_markdown = result.markdown
            else:
                clean_markdown = str(result.markdown)

        # åå¤„ç†æ¸…ç†
        if clean_markdown:
            clean_markdown = _post_process_markdown(clean_markdown)

        word_count = len(clean_markdown.split()) if clean_markdown else 0

        return MarkdownData(
            url=request.url,
            status_code=getattr(result, 'status_code', None),
            raw_markdown=None,  # è¶…çº§æ¸…ç†æ¨¡å¼åªè¿”å›æ¸…ç†åçš„å†…å®¹
            fit_markdown=clean_markdown,
            title=title,
            word_count=word_count
        )


async def _crawl_markdown_with_query(request: MarkdownRequest, query: str) -> MarkdownData:
    """ä½¿ç”¨BM25æŸ¥è¯¢è¿‡æ»¤çš„å†…éƒ¨å‡½æ•°"""
    from crawl4ai.content_filter_strategy import BM25ContentFilter
    from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

    # BM25æŸ¥è¯¢é…ç½®
    bm25_filter = BM25ContentFilter(
        user_query=query,
        bm25_threshold=0.8,  # è¾ƒä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šç›¸å…³å†…å®¹
    )

    query_options = {
        "ignore_links": True,
        "escape_html": False,
        "body_width": 0,
        "unicode_snob": True,
        "mark_code": True,
        "default_image_alt": "[å›¾ç‰‡]",
    }

    md_generator = DefaultMarkdownGenerator(
        content_filter=bm25_filter,
        options=query_options
    )

    browser_config = crawler_service._create_browser_config(request.js_enabled)
    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS if request.bypass_cache else CacheMode.ENABLED,
        page_timeout=90000,
        markdown_generator=md_generator,
        wait_for_images=True,
    )

    if request.css_selector:
        config.css_selector = request.css_selector

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=request.url, config=config)

        if not result.success:
            raise CrawlerException(
                message=getattr(result, 'error_message', 'BM25æŸ¥è¯¢æ¨¡å¼è·å–å¤±è´¥'),
                error_type="crawl_failed"
            )

        # è§£æç»“æœ
        title = None
        if hasattr(result, 'metadata') and result.metadata:
            title = result.metadata.get('title')

        # è·å–æŸ¥è¯¢ç›¸å…³çš„å†…å®¹
        query_markdown = None
        if hasattr(result, 'markdown'):
            if hasattr(result.markdown, 'fit_markdown'):
                query_markdown = result.markdown.fit_markdown
            elif isinstance(result.markdown, str):
                query_markdown = result.markdown
            else:
                query_markdown = str(result.markdown)

        # è½»åº¦åå¤„ç†ï¼ˆä¿ç•™æ›´å¤šå†…å®¹ï¼Œå› ä¸ºæ˜¯æŸ¥è¯¢ç›¸å…³çš„ï¼‰
        if query_markdown:
            query_markdown = _light_post_process_markdown(query_markdown)

        word_count = len(query_markdown.split()) if query_markdown else 0

        return MarkdownData(
            url=request.url,
            status_code=getattr(result, 'status_code', None),
            raw_markdown=None,
            fit_markdown=query_markdown,
            title=title,
            word_count=word_count
        )


def _post_process_markdown(markdown: str) -> str:
    """
    æ¿€è¿›çš„åå¤„ç†æ¸…ç† Markdown å†…å®¹
    """
    import re

    if not markdown:
        return ""

    # ç§»é™¤å¯¼èˆªå’Œåˆ†äº«ç›¸å…³å†…å®¹
    navigation_patterns = [
        r'\[Skip to .*?\]\(.*?\)',
        r'\[Accessibility help\]\(.*?\)',
        r'current progress \d+%',
        r'\[.*? on (x|facebook|linkedin|whatsapp).*?\]\(.*?\)',
        r'Jump to comments section',
        r'Print this page',
        r'Reuse this content.*?\n',
        r'Close side navigation menu',
        r'Subscribe for full access',
        r'Follow the topics in this article',
        r'Promoted Content',
        r'Comments\n',
        r'\*\[.*?\]: .*\n',  # ç§»é™¤å¼•ç”¨å®šä¹‰
    ]

    for pattern in navigation_patterns:
        markdown = re.sub(pattern, '', markdown, flags=re.IGNORECASE)

    # ç§»é™¤é‡å¤çš„ç¤¾äº¤åˆ†äº«é“¾æ¥å—
    markdown = re.sub(
        r'(\* \[.*? on (x|facebook|linkedin|whatsapp).*?\]\(.*?\)\n){2,}', '', markdown, flags=re.IGNORECASE)

    # ç§»é™¤ç©ºçš„åˆ—è¡¨é¡¹
    markdown = re.sub(r'^\s*\*\s*$', '', markdown, flags=re.MULTILINE)

    # æ¸…ç†å¤šä½™çš„ç©ºç™½
    markdown = re.sub(r'\n\s*\n\s*\n+', '\n\n', markdown)
    markdown = re.sub(r'[ \t]+', ' ', markdown)

    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½
    markdown = markdown.strip()

    return markdown


def _light_post_process_markdown(markdown: str) -> str:
    """
    è½»åº¦åå¤„ç† - ä¿ç•™æ›´å¤šå†…å®¹ï¼Œåªç§»é™¤æ˜æ˜¾çš„å™ªéŸ³
    """
    import re

    if not markdown:
        return ""

    # åªç§»é™¤æœ€æ˜æ˜¾çš„å¯¼èˆªå…ƒç´ 
    light_patterns = [
        r'\[Skip to .*?\]\(.*?\)',
        r'\[Accessibility help\]\(.*?\)',
        r'current progress \d+%',
    ]

    for pattern in light_patterns:
        markdown = re.sub(pattern, '', markdown, flags=re.IGNORECASE)

    # æ¸…ç†å¤šä½™çš„ç©ºç™½
    markdown = re.sub(r'\n\s*\n\s*\n+', '\n\n', markdown)
    markdown = markdown.strip()

    return markdown


@router.post("/url", response_model=CrawlResponse)
async def crawl_single_url(request: CrawlRequest) -> CrawlResponse:
    """
    çˆ¬å–å•ä¸ªURLå¹¶è¿”å›å®Œæ•´ç»“æœ
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        data = await crawler_service.crawl_url(request)
        return ApiResponse.success_response(
            data=data,
            message="çˆ¬å–æˆåŠŸ"
        )

    except CrawlerException as e:
        # æ ¹æ®å¼‚å¸¸ç±»å‹æ˜ å°„é”™è¯¯ç 
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "crawl_failed": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.post("/markdown", response_model=MarkdownResponse)
async def crawl_markdown(request: MarkdownRequest) -> MarkdownResponse:
    """
    ä¸“é—¨è·å–é¡µé¢çš„Markdownå†…å®¹

    æ”¯æŒå¤šç§æ ¼å¼:
    - raw: åŸå§‹markdownå†…å®¹
    - fit: ç»è¿‡å†…å®¹è¿‡æ»¤çš„markdown,æ›´é€‚åˆAIå¤„ç†
    - both: åŒæ—¶è¿”å›ä¸¤ç§æ ¼å¼
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        data = await crawler_service.crawl_markdown(request)
        return ApiResponse.success_response(
            data=data,
            message="Markdownè·å–æˆåŠŸ"
        )

    except CrawlerException as e:
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "crawl_failed": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.post("/screenshot", response_model=ScreenshotResponse)
async def take_screenshot(request: ScreenshotRequest) -> ScreenshotResponse:
    """
    æˆªå–é¡µé¢æˆªå›¾

    æ”¯æŒåŠŸèƒ½:
    - å…¨é¡µé¢æˆªå›¾æˆ–æŒ‡å®šCSSé€‰æ‹©å™¨åŒºåŸŸæˆªå›¾
    - è‡ªå®šä¹‰è§†çª—å¤§å°
    - JavaScriptæ‰§è¡Œæ§åˆ¶
    - ç¼“å­˜æ§åˆ¶
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        data = await crawler_service.take_screenshot(request)
        return ApiResponse.success_response(
            data=data,
            message="æˆªå›¾æˆåŠŸ"
        )

    except CrawlerException as e:
        # æ ¹æ®å¼‚å¸¸ç±»å‹æ˜ å°„é”™è¯¯ç 
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "screenshot_failed": BusinessCode.CRAWL_FAILED,
            "screenshot_empty": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.get("/health", response_model=HealthResponse)
async def health_check() -> HealthResponse:
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    return ApiResponse.success_response(
        data={"status": "healthy", "service": "crawl4ai-api"},
        message="æœåŠ¡æ­£å¸¸"
    )


# åœ¨ app/routers/crawls.py ä¸­æ·»åŠ è‡ªåŠ¨é…ç½®æ’ä»¶çš„åŠŸèƒ½

@router.post("/auto-configure-extension", response_model=ApiResponse[dict])
async def auto_configure_extension() -> ApiResponse[dict]:
    """
    è‡ªåŠ¨é…ç½® Bypass Paywalls Clean æ’ä»¶
    å¯ç”¨ custom sites å’Œå…¶ä»–å¿…è¦è®¾ç½®ï¼Œä¸€æ¬¡æ€§é…ç½®ï¼Œæ°¸ä¹…ç”Ÿæ•ˆ
    """
    try:
        # æ£€æŸ¥æ‰©å±•è·¯å¾„
        extension_path = None
        env_path = os.environ.get('CHROME_EXTENSION_PATH')
        if env_path and os.path.exists(env_path):
            extension_path = env_path
        else:
            project_extension_path = Path(
                "./chrome-extension/bypass-paywalls-chrome-clean")
            if project_extension_path.exists():
                extension_path = str(project_extension_path.resolve())

        if not extension_path:
            return ApiResponse.error_response(
                code=BusinessCode.INTERNAL_ERROR,
                message="æœªæ£€æµ‹åˆ°æ‰©å±•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„"
            )

        # ä½¿ç”¨ä¸æ­£å¸¸çˆ¬å–ç›¸åŒçš„æŒä¹…åŒ–é…ç½®
        browser_config = BrowserConfig(
            headless=False,  # éœ€è¦å¯è§æ¨¡å¼æ¥é…ç½®
            java_script_enabled=True,
            viewport={"width": 1280, "height": 800},
            verbose=True,
            user_data_dir="./extension_browser_profile",  # ä¸æ­£å¸¸çˆ¬å–ç›¸åŒçš„ç›®å½•
            use_persistent_context=True,
            extra_args=[
                f"--load-extension={extension_path}",
                f"--disable-extensions-except={extension_path}",
                "--disable-extensions-except-devtools",
                "--enable-extensions",
                "--no-first-run",
                "--no-default-browser-check",
                "--disable-default-apps",
            ]
        )

        config_result = {
            "extension_path": extension_path,
            "user_data_dir": "./extension_browser_profile",
            "steps_completed": [],
            "configuration_successful": False
        }

        async with AsyncWebCrawler(config=browser_config) as crawler:
            print("ğŸš€ å¼€å§‹è‡ªåŠ¨é…ç½®æ’ä»¶...")

            # ç¬¬ä¸€æ­¥ï¼šè®¿é—®æ’ä»¶é…ç½®é¡µé¢
            print("ğŸ“‹ æ­£åœ¨æ‰“å¼€æ’ä»¶é…ç½®é¡µé¢...")
            extension_id = "lkbebcjgcmobigpeffafkodonchffocl"  # Bypass Paywalls Clean çš„æ‰©å±•ID
            config_url = f"chrome-extension://{extension_id}/options/options.html"

            config_page_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=15000,
                js_code="""
                // è‡ªåŠ¨é…ç½®æ’ä»¶çš„JavaScriptä»£ç 
                async function autoConfigureExtension() {
                    console.log('ğŸ”§ å¼€å§‹è‡ªåŠ¨é…ç½®æ’ä»¶...');

                    // ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                    await new Promise(resolve => setTimeout(resolve, 2000));

                    let configuredOptions = [];

                    // 1. å¯ç”¨ custom sites
                    const customSitesEnableBtn = document.querySelector('button[onclick="enable_custom_sites()"]');
                    if (customSitesEnableBtn && customSitesEnableBtn.textContent.includes('Enable')) {
                        customSitesEnableBtn.click();
                        configuredOptions.push('custom_sites_enabled');
                        console.log('âœ… Custom sites å·²å¯ç”¨');
                        await new Promise(resolve => setTimeout(resolve, 1000));
                    }

                    // 2. å¯ç”¨ setCookie opt-in (å¦‚æœéœ€è¦)
                    const setCookieEnableBtn = document.querySelector('button[onclick="enable_setCookie()"]');
                    if (setCookieEnableBtn && setCookieEnableBtn.textContent.includes('Enable')) {
                        setCookieEnableBtn.click();
                        configuredOptions.push('setCookie_enabled');
                        console.log('âœ… setCookie å·²å¯ç”¨');
                        await new Promise(resolve => setTimeout(resolve, 1000));
                    }

                    // 3. æ£€æŸ¥é…ç½®çŠ¶æ€
                    const customSitesStatus = document.querySelector('body').textContent.includes('custom sites enabled: YES');
                    const setCookieStatus = document.querySelector('body').textContent.includes('setCookie opt-in enabled: YES');

                    console.log('ğŸ“Š é…ç½®çŠ¶æ€æ£€æŸ¥:');
                    console.log('   - Custom sites:', customSitesStatus ? 'YES' : 'NO');
                    console.log('   - setCookie:', setCookieStatus ? 'YES' : 'NO');

                    // å°†ç»“æœå­˜å‚¨åˆ°é¡µé¢ï¼Œä¾›çˆ¬è™«è¯»å–
                    const resultDiv = document.createElement('div');
                    resultDiv.id = 'auto-config-result';
                    resultDiv.style.display = 'none';
                    resultDiv.textContent = JSON.stringify({
                        configuredOptions: configuredOptions,
                        customSitesEnabled: customSitesStatus,
                        setCookieEnabled: setCookieStatus,
                        success: configuredOptions.length > 0 || (customSitesStatus && setCookieStatus)
                    });
                    document.body.appendChild(resultDiv);

                    console.log('ğŸ‰ è‡ªåŠ¨é…ç½®å®Œæˆ!');
                    return configuredOptions;
                }

                // æ‰§è¡Œé…ç½®
                autoConfigureExtension();
                """
            )

            result = await crawler.arun(url=config_url, config=config_page_config)

            if result.success:
                config_result["steps_completed"].append("opened_config_page")

                # ä»é¡µé¢ä¸­æå–é…ç½®ç»“æœ
                try:
                    import re
                    import json

                    # æŸ¥æ‰¾ç»“æœæ•°æ®
                    result_match = re.search(
                        r'<div id="auto-config-result"[^>]*>([^<]+)</div>', result.html)
                    if result_match:
                        result_data = json.loads(result_match.group(1))
                        config_result.update({
                            "configured_options": result_data.get("configuredOptions", []),
                            "custom_sites_enabled": result_data.get("customSitesEnabled", False),
                            "setCookie_enabled": result_data.get("setCookieEnabled", False),
                            "configuration_successful": result_data.get("success", False)
                        })
                        config_result["steps_completed"].append(
                            "auto_configuration_executed")

                    # æ‰‹åŠ¨æ£€æŸ¥é¡µé¢å†…å®¹
                    page_content = result.html.lower()
                    if "custom sites enabled: yes" in page_content:
                        config_result["custom_sites_enabled"] = True
                    if "setcookie opt-in enabled: yes" in page_content:
                        config_result["setCookie_enabled"] = True

                except Exception as e:
                    print(f"âš ï¸ è§£æé…ç½®ç»“æœæ—¶å‡ºé”™: {str(e)}")
                    config_result["parse_error"] = str(e)

            # ç¬¬äºŒæ­¥ï¼šæµ‹è¯•é…ç½®æ˜¯å¦ç”Ÿæ•ˆ
            print("ğŸ§ª æµ‹è¯•æ’ä»¶é…ç½®...")
            test_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=10000
            )

            test_result = await crawler.arun(url="https://httpbin.org/get", config=test_config)
            if test_result.success:
                config_result["steps_completed"].append(
                    "test_navigation_successful")

            # ä¿æŒæµè§ˆå™¨æ‰“å¼€è®©ç”¨æˆ·ç¡®è®¤
            print("ğŸ” ä¿æŒæµè§ˆå™¨æ‰“å¼€15ç§’ä¾›æ‚¨ç¡®è®¤é…ç½®...")
            print("   è¯·åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹æ’ä»¶é…ç½®é¡µé¢ç¡®è®¤è®¾ç½®")
            await asyncio.sleep(15)

        # æ€»ç»“é…ç½®ç»“æœ
        success_indicators = [
            config_result.get("custom_sites_enabled", False),
            config_result.get("setCookie_enabled", False),
            len(config_result.get("configured_options", [])) > 0
        ]

        overall_success = any(success_indicators)
        config_result["overall_success"] = overall_success

        if overall_success:
            message = "ğŸ‰ æ’ä»¶è‡ªåŠ¨é…ç½®å®Œæˆï¼è®¾ç½®å·²æ°¸ä¹…ä¿å­˜ï¼Œåç»­çˆ¬å–æ— éœ€å†é…ç½®"
        else:
            message = "âš ï¸ è‡ªåŠ¨é…ç½®å¯èƒ½æœªå®Œå…¨æˆåŠŸï¼Œå»ºè®®æ‰‹åŠ¨æ£€æŸ¥æ’ä»¶è®¾ç½®"

        return ApiResponse.success_response(
            data=config_result,
            message=message
        )

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"è‡ªåŠ¨é…ç½®æ’ä»¶å¤±è´¥: {str(e)}"
        )


@router.post("/debug/extension", response_model=ApiResponse[dict])
async def debug_extension_loading(request: CrawlRequest) -> ApiResponse[dict]:
    """
    è°ƒè¯•æ¥å£ï¼šä¸“ç”¨äºæµ‹è¯•æ‰©å±•åŠŸèƒ½ - å¼ºåˆ¶å¯è§æ¨¡å¼
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        # æ£€æŸ¥æ‰©å±•è·¯å¾„
        extension_path = None
        env_path = os.environ.get('CHROME_EXTENSION_PATH')
        if env_path and os.path.exists(env_path):
            extension_path = env_path
        else:
            project_extension_path = Path(
                "./chrome-extension/bypass-paywalls-chrome-clean")
            if project_extension_path.exists():
                extension_path = str(project_extension_path.resolve())

        debug_info = {
            "extension_detected": extension_path is not None,
            "extension_path": extension_path,
            "url_tested": request.url,
            "debug_mode": True,
            "forced_visible": True,
        }

        if not extension_path:
            return ApiResponse.success_response(
                data=debug_info,
                message="âš ï¸ æœªæ£€æµ‹åˆ°æ‰©å±•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„"
            )

        # ğŸ”§ è°ƒè¯•ä¸“ç”¨é…ç½®ï¼šå¼ºåˆ¶å¯è§æ¨¡å¼
        browser_config = BrowserConfig(
            headless=False,  # è°ƒè¯•å¼ºåˆ¶å¯è§
            java_script_enabled=request.js_enabled,
            viewport={"width": 1280, "height": 800},
            verbose=True,
            user_data_dir="./extension_browser_profile",
            use_persistent_context=True,
            extra_args=[
                f"--load-extension={extension_path}",
                f"--disable-extensions-except={extension_path}",
                "--disable-extensions-except-devtools",
                "--enable-extensions",
                "--no-first-run",
                "--no-default-browser-check",
                "--disable-default-apps",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
            ]
        )

        print("ğŸš€ å¯åŠ¨è°ƒè¯•æ¨¡å¼ï¼ˆå¼ºåˆ¶å¯è§ï¼‰...")

        async with AsyncWebCrawler(config=browser_config) as crawler:
            try:
                print("=" * 60)
                print("ğŸ” è°ƒè¯•æ¨¡å¼è¯´æ˜ï¼š")
                print("   - æµè§ˆå™¨å°†ä¿æŒå¯è§çŠ¶æ€")
                print("   - å¯ä»¥æ‰‹åŠ¨æ£€æŸ¥æ’ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ")
                print("   - 10ç§’åå°†è®¿é—®ç›®æ ‡URL")
                print("=" * 60)

                await asyncio.sleep(10)

                print("ğŸŒ å¼€å§‹è®¿é—®ç›®æ ‡URL...")

                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    page_timeout=60000,
                    wait_for_images=request.include_images,
                )

                if request.css_selector:
                    config.css_selector = request.css_selector

                # ğŸ”§ ç®€åŒ–çš„é¢„çƒ­ï¼šåªç”¨HTTPè¯·æ±‚
                print("ğŸ”¥ æ‰©å±•é¢„çƒ­ä¸­...")
                try:
                    warmup_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        page_timeout=10000
                    )
                    await crawler.arun(url="https://httpbin.org/get", config=warmup_config)
                    await asyncio.sleep(2)
                    print("âœ… æ‰©å±•é¢„çƒ­å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ é¢„çƒ­å¤±è´¥ï¼Œç»§ç»­æµ‹è¯•: {str(e)}")

                # æ‰§è¡Œç›®æ ‡URLæŠ“å–
                result = await crawler.arun(url=request.url, config=config)

                print(
                    f"ğŸ“„ é¡µé¢åŠ è½½å®Œæˆï¼ŒçŠ¶æ€ç : {getattr(result, 'status_code', 'Unknown')}")

                # åˆ†æç»“æœ
                debug_info.update({
                    "crawl_success": result.success,
                    "status_code": getattr(result, 'status_code', None),
                    "content_length": len(result.markdown) if result.markdown else 0,
                    "content_preview": result.markdown[:300] if result.markdown else "No content",
                })

                # ä»˜è´¹å¢™æ£€æµ‹
                if result.markdown:
                    content_lower = result.markdown.lower()
                    paywall_indicators = [
                        "subscribe", "sign in", "premium", "subscription", "paywall"]
                    detected_indicators = [
                        ind for ind in paywall_indicators if ind in content_lower]

                    debug_info["paywall_indicators_found"] = detected_indicators
                    debug_info["paywall_indicators_count"] = len(
                        detected_indicators)
                    debug_info["likely_success"] = len(
                        detected_indicators) <= 2

                    if len(detected_indicators) <= 1:
                        print("ğŸ‰ ä¼˜ç§€ï¼å‡ ä¹æ²¡æœ‰ä»˜è´¹å¢™æŒ‡æ ‡")
                        debug_info["quality_assessment"] = "excellent"
                    elif len(detected_indicators) <= 2:
                        print("ğŸŸ¢ è‰¯å¥½ï¼å°‘é‡ä»˜è´¹å¢™æŒ‡æ ‡")
                        debug_info["quality_assessment"] = "good"
                    elif len(detected_indicators) <= 3:
                        print("ğŸŸ¡ ä¸­ç­‰ï¼Œå­˜åœ¨ä¸€äº›ä»˜è´¹å¢™æŒ‡æ ‡")
                        debug_info["quality_assessment"] = "medium"
                    else:
                        print(f"ğŸ”´ è¾ƒå·®ï¼Œæ£€æµ‹åˆ°è¾ƒå¤šä»˜è´¹å¢™æŒ‡æ ‡: {detected_indicators}")
                        debug_info["quality_assessment"] = "poor"

                if not result.success:
                    debug_info["error_message"] = getattr(
                        result, 'error_message', 'æœªçŸ¥é”™è¯¯')

                print("ğŸ” ä¿æŒæµè§ˆå™¨æ‰“å¼€20ç§’ä¾›æ‚¨æ£€æŸ¥ç»“æœ...")
                print("   æ‚¨å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨æŸ¥çœ‹é¡µé¢å†…å®¹")
                await asyncio.sleep(20)

            except Exception as e:
                print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                debug_info["execution_error"] = str(e)
                await asyncio.sleep(10)

        print("ğŸ”š è°ƒè¯•ä¼šè¯ç»“æŸ")

        # ç”Ÿæˆå»ºè®®æ¶ˆæ¯
        quality = debug_info.get("quality_assessment", "unknown")
        if quality == "excellent":
            message = "ğŸ‰ è°ƒè¯•æˆåŠŸï¼æ‰©å±•å·¥ä½œå®Œç¾ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ç”Ÿäº§æ¥å£"
        elif quality == "good":
            message = "âœ… è°ƒè¯•æˆåŠŸï¼æ‰©å±•å·¥ä½œè‰¯å¥½ï¼Œå»ºè®®ä½¿ç”¨ç”Ÿäº§æ¥å£"
        elif quality == "medium":
            message = "ğŸŸ¡ è°ƒè¯•æ˜¾ç¤ºä¸­ç­‰æ•ˆæœï¼Œå¯ä»¥å°è¯•ç”Ÿäº§æ¥å£ä½†å¯èƒ½éœ€è¦ä¼˜åŒ–"
        else:
            message = "ğŸ” è°ƒè¯•å®Œæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†ç»“æœå¹¶è€ƒè™‘é…ç½®ä¼˜åŒ–"

        return ApiResponse.success_response(
            data=debug_info,
            message=message
        )

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"è°ƒè¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        )


@router.post("/test/headless-mode", response_model=ApiResponse[dict])
async def test_headless_mode(request: CrawlRequest) -> ApiResponse[dict]:
    """
    æµ‹è¯•æ— å¤´æ¨¡å¼æ•ˆæœ - å¯¹æ¯”æœ‰å¤´å’Œæ— å¤´æ¨¡å¼çš„å·®å¼‚
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        from app.models.models import MarkdownRequest, MarkdownFormat

        # è½¬æ¢ä¸ºMarkdownRequest
        markdown_request = MarkdownRequest(
            url=request.url,
            format=MarkdownFormat.FIT,
            js_enabled=request.js_enabled,
            bypass_cache=request.bypass_cache,
            css_selector=request.css_selector
        )

        test_results = {
            "url": request.url,
            "tests_performed": [],
            "comparison": {}
        }

        try:
            # æµ‹è¯•1ï¼šæ— å¤´æ¨¡å¼
            print("ğŸ¤– æµ‹è¯•æ— å¤´æ¨¡å¼...")
            headless_result = await crawler_service._crawl_markdown_with_mode(markdown_request, headless=True)

            headless_analysis = {
                "success": True,
                "word_count": headless_result.word_count,
                "status_code": headless_result.status_code,
                "content_preview": (headless_result.fit_markdown or headless_result.raw_markdown or "")[:200]
            }

            # åˆ†æä»˜è´¹å¢™æŒ‡æ ‡
            content = headless_result.fit_markdown or headless_result.raw_markdown or ""
            paywall_indicators = ["subscribe", "sign in",
                                  "premium", "subscription", "paywall"]
            headless_indicators = [
                ind for ind in paywall_indicators if ind in content.lower()]
            headless_analysis["paywall_indicators"] = len(headless_indicators)

            test_results["headless_mode"] = headless_analysis
            test_results["tests_performed"].append("headless")

        except Exception as e:
            test_results["headless_mode"] = {
                "success": False,
                "error": str(e)
            }

        try:
            # æµ‹è¯•2ï¼šå¯è§æ¨¡å¼
            print("ğŸ‘ï¸ æµ‹è¯•å¯è§æ¨¡å¼...")
            visible_result = await crawler_service._crawl_markdown_with_mode(markdown_request, headless=False)

            visible_analysis = {
                "success": True,
                "word_count": visible_result.word_count,
                "status_code": visible_result.status_code,
                "content_preview": (visible_result.fit_markdown or visible_result.raw_markdown or "")[:200]
            }

            # åˆ†æä»˜è´¹å¢™æŒ‡æ ‡
            content = visible_result.fit_markdown or visible_result.raw_markdown or ""
            paywall_indicators = ["subscribe", "sign in",
                                  "premium", "subscription", "paywall"]
            visible_indicators = [
                ind for ind in paywall_indicators if ind in content.lower()]
            visible_analysis["paywall_indicators"] = len(visible_indicators)

            test_results["visible_mode"] = visible_analysis
            test_results["tests_performed"].append("visible")

        except Exception as e:
            test_results["visible_mode"] = {
                "success": False,
                "error": str(e)
            }

        # å¯¹æ¯”åˆ†æ
        if "headless" in test_results["tests_performed"] and "visible" in test_results["tests_performed"]:
            headless_data = test_results["headless_mode"]
            visible_data = test_results["visible_mode"]

            if headless_data["success"] and visible_data["success"]:
                comparison = {
                    "word_count_diff": visible_data["word_count"] - headless_data["word_count"],
                    "paywall_indicators_diff": visible_data["paywall_indicators"] - headless_data["paywall_indicators"],
                    "headless_quality": "good" if headless_data["paywall_indicators"] <= 2 else "poor",
                    "visible_quality": "good" if visible_data["paywall_indicators"] <= 2 else "poor",
                    "recommendation": ""
                }

                if headless_data["paywall_indicators"] <= 2:
                    comparison["recommendation"] = "æ— å¤´æ¨¡å¼æ•ˆæœè‰¯å¥½ï¼Œæ¨èç”Ÿäº§ç¯å¢ƒä½¿ç”¨"
                elif visible_data["paywall_indicators"] <= 2:
                    comparison["recommendation"] = "å»ºè®®ä½¿ç”¨å¯è§æ¨¡å¼ï¼Œæˆ–ä¼˜åŒ–æ— å¤´æ¨¡å¼é…ç½®"
                else:
                    comparison["recommendation"] = "ä¸¤ç§æ¨¡å¼æ•ˆæœéƒ½ä¸ç†æƒ³ï¼Œå»ºè®®æ£€æŸ¥æ‰©å±•é…ç½®"

                test_results["comparison"] = comparison

        return ApiResponse.success_response(
            data=test_results,
            message="ğŸ§ª æ— å¤´/å¯è§æ¨¡å¼å¯¹æ¯”æµ‹è¯•å®Œæˆ"
        )

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        )


@router.post("/markdown/clean", response_model=MarkdownResponse)
async def crawl_markdown_clean(request: MarkdownRequest) -> MarkdownResponse:
    """
    è·å–è¶…çº§å¹²å‡€çš„ Markdown å†…å®¹ - ä¸“é—¨ç”¨äº LLM æ¶ˆè´¹

    ä½¿ç”¨æœ€æ¿€è¿›çš„è¿‡æ»¤è®¾ç½®ï¼Œå»é™¤æ‰€æœ‰é“¾æ¥ã€å¯¼èˆªã€å¹¿å‘Šç­‰å™ªéŸ³
    ä¸“é—¨ä¼˜åŒ–ç”¨äºAIå¤„ç†çš„å†…å®¹
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        # ğŸ”§ åˆ›å»ºè¶…çº§æ¸…ç†ç‰ˆæœ¬çš„è¯·æ±‚
        clean_request = MarkdownRequest(
            url=request.url,
            format=MarkdownFormat.FIT,  # å¼ºåˆ¶ä½¿ç”¨fitæ¨¡å¼
            js_enabled=request.js_enabled,
            bypass_cache=request.bypass_cache,
            css_selector=request.css_selector,
            ignore_links=True,  # å¼ºåˆ¶å¿½ç•¥é“¾æ¥
            escape_html=False,
            body_width=0,
        )

        # ä½¿ç”¨ä¸“é—¨çš„æ¸…ç†é…ç½®
        data = await _crawl_markdown_with_clean_config(clean_request)

        return ApiResponse.success_response(
            data=data,
            message="è¶…çº§æ¸…ç†æ¨¡å¼ Markdown è·å–æˆåŠŸ"
        )

    except CrawlerException as e:
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "crawl_failed": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"è¶…çº§æ¸…ç†æ¨¡å¼å¤±è´¥: {str(e)}"
        )


@router.post("/markdown/query", response_model=MarkdownResponse)
async def crawl_markdown_with_query(
    request: MarkdownRequest,
    query: str = Query(..., description="æœç´¢æŸ¥è¯¢ï¼Œç”¨äºBM25å†…å®¹è¿‡æ»¤")
) -> MarkdownResponse:
    """
    åŸºäºæŸ¥è¯¢çš„æ™ºèƒ½ Markdown æå–

    ä½¿ç”¨ BM25 ç®—æ³•æ ¹æ®æŸ¥è¯¢å†…å®¹æå–æœ€ç›¸å…³çš„éƒ¨åˆ†
    ä¾‹å¦‚: query="Trump China trade" å°†æå–ä¸è´¸æ˜“ç›¸å…³çš„å†…å®¹
    """
    if not is_valid_url(request.url):
        return ApiResponse.error_response(
            code=BusinessCode.INVALID_URL,
            message="æ— æ•ˆçš„URLæ ¼å¼"
        )

    try:
        # åˆ›å»ºå¸¦æŸ¥è¯¢çš„è¯·æ±‚
        query_request = MarkdownRequest(
            url=request.url,
            format=MarkdownFormat.FIT,
            js_enabled=request.js_enabled,
            bypass_cache=request.bypass_cache,
            css_selector=request.css_selector,
            ignore_links=True,
            escape_html=False,
        )

        data = await _crawl_markdown_with_query(query_request, query)

        return ApiResponse.success_response(
            data=data,
            message=f"åŸºäºæŸ¥è¯¢ '{query}' çš„æ™ºèƒ½æå–å®Œæˆ"
        )

    except CrawlerException as e:
        error_code_map = {
            "timeout": BusinessCode.CRAWL_TIMEOUT,
            "crawl_failed": BusinessCode.CRAWL_FAILED,
            "unexpected": BusinessCode.INTERNAL_ERROR
        }

        code = error_code_map.get(e.error_type, BusinessCode.CRAWL_FAILED)
        return ApiResponse.error_response(code=code, message=e.message)

    except Exception as e:
        return ApiResponse.error_response(
            code=BusinessCode.INTERNAL_ERROR,
            message=f"æ™ºèƒ½æŸ¥è¯¢æå–å¤±è´¥: {str(e)}"
        )

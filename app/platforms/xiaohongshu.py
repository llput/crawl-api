# app/platforms/xiaohongshu.py (å¢å¼ºç‰ˆæœ¬)
import re
import logging
import asyncio
from typing import Dict, List, Optional, Any
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from datetime import datetime

from .base import BasePlatform, PlatformConfig
from app.services.crawler_service import CrawlerException
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

logger = logging.getLogger(__name__)


class XiaohongshuPlatform(BasePlatform):
    """å°çº¢ä¹¦å¹³å°å®ç° - å¢å¼ºç‰ˆæœ¬ï¼Œä¸“é—¨å¤„ç†å°çº¢ä¹¦çš„ç‰¹æ®Šéœ€æ±‚"""

    def __init__(self, auth_service):
        super().__init__(auth_service)
        self._token_cache = {}  # ç®€å•çš„tokenç¼“å­˜
        self._cache_ttl = 300   # 5åˆ†é’Ÿç¼“å­˜

    def get_config(self) -> PlatformConfig:
        return PlatformConfig(
            name="xiaohongshu",
            display_name="å°çº¢ä¹¦",
            enabled=True,
            site_name="xiaohongshu_com",
            default_source_url="https://www.xiaohongshu.com/explore",
            version="1.1.0"
        )

    async def crawl_content_by_id(
        self,
        content_id: str,
        source_url: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """çˆ¬å–å°çº¢ä¹¦ç¬”è®°å†…å®¹ - å¢å¼ºç‰ˆæœ¬"""
        try:
            logger.info(f"ğŸ“ æ­£åœ¨çˆ¬å–å°çº¢ä¹¦ç¬”è®°: {content_id}")

            # ç¬¬ä¸€æ­¥ï¼šè·å–æœ‰æ•ˆçš„è®¿é—®é“¾æ¥
            target_url = await self._get_note_access_url(content_id, source_url)

            if not target_url:
                raise CrawlerException(
                    message=f"æ— æ³•è·å–ç¬”è®° {content_id} çš„æœ‰æ•ˆè®¿é—®é“¾æ¥",
                    error_type="link_not_found"
                )

            logger.info(f"ğŸ”— æ„é€ çš„è®¿é—®é“¾æ¥: {target_url}")

            # ç¬¬äºŒæ­¥ï¼šçˆ¬å–ç¬”è®°å†…å®¹
            browser_config = await self._get_xiaohongshu_browser_config()
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=60000,
                wait_for_images=True,
            )

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=target_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"çˆ¬å–ç¬”è®°å†…å®¹å¤±è´¥: {result.error_message}",
                        error_type="crawl_failed"
                    )

                # ç¬¬ä¸‰æ­¥ï¼šè§£æå’Œæ ¼å¼åŒ–æ•°æ®
                note_data = self._parse_xiaohongshu_note(
                    result, content_id, target_url
                )

                logger.info(f"âœ… å°çº¢ä¹¦ç¬”è®° {content_id} çˆ¬å–æˆåŠŸ")
                return note_data

        except CrawlerException:
            raise
        except Exception as e:
            logger.error(f"âŒ å°çº¢ä¹¦ç¬”è®°çˆ¬å–å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"å°çº¢ä¹¦ç¬”è®° {content_id} çˆ¬å–å¤±è´¥: {str(e)}",
                error_type="crawl_failed"
            )

    async def _get_note_access_url(
        self,
        note_id: str,
        source_url: Optional[str] = None
    ) -> Optional[str]:
        """è·å–ç¬”è®°çš„æœ‰æ•ˆè®¿é—®URL - å¢å¼ºç‰ˆæœ¬"""

        logger.info(f"ğŸ¯ æ­£åœ¨è·å–ç¬”è®° {note_id} çš„è®¿é—®URL")

        # æ–¹æ³•1ï¼šä»ç¼“å­˜çš„tokenæ„é€ 
        cached_token = self._get_cached_token()
        if cached_token:
            url = self._build_note_url_with_token(note_id, cached_token)
            if url:
                logger.info(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜tokenæ„é€ URL: {url}")
                return url

        # æ–¹æ³•2ï¼šä»æ¢ç´¢é¡µæå–æ–°token
        try:
            logger.info("ğŸ” ä»æ¢ç´¢é¡µæå–æ–°çš„token...")
            links_data = await self.extract_content_links(source_url, max_links=50)

            # ğŸ”§ ä¼˜å…ˆæŸ¥æ‰¾å‚æ•°å®Œæ•´çš„é“¾æ¥
            complete_links = [note for note in links_data["notes"]
                              if note.get("complete_params", False)]
            target_links = complete_links if complete_links else links_data["notes"]

            # æŸ¥æ‰¾ç›®æ ‡ç¬”è®°çš„ç›´æ¥é“¾æ¥
            for note in target_links:
                if note["note_id"] == note_id:
                    self._cache_token_from_url(note["url"])
                    logger.info(f"ğŸ¯ æ‰¾åˆ°ç›®æ ‡ç¬”è®°é“¾æ¥: {note['url']}")
                    return note["url"]

            # å¦‚æœæ²¡æ‰¾åˆ°ç›®æ ‡ç¬”è®°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå®Œæ•´å‚æ•°çš„é“¾æ¥æ„é€ 
            if target_links:
                first_note_url = target_links[0]["url"]
                token_info = self._extract_token_from_url(first_note_url)
                if token_info:
                    self._cache_token(token_info)
                    constructed_url = self._build_note_url_with_token(
                        note_id, token_info)
                    logger.info(f"ğŸ”¨ ä½¿ç”¨ç¬¬ä¸€ä¸ªé“¾æ¥çš„tokenæ„é€ : {constructed_url}")
                    return constructed_url

        except Exception as e:
            logger.warning(f"âš ï¸ ä»æ¢ç´¢é¡µè·å–tokenå¤±è´¥: {str(e)}")

        return None

    def _extract_xiaohongshu_notes_from_html(
        self,
        html: str,
        base_url: str,
        max_links: int
    ) -> List[Dict[str, Any]]:
        """ä»HTMLä¸­æå–å°çº¢ä¹¦ç¬”è®°é“¾æ¥ - ä¿®å¤ç‰ˆæœ¬"""
        notes = []

        # ğŸ” å…ˆæ£€æŸ¥HTMLåŸºæœ¬å†…å®¹
        if not html or len(html) < 1000:
            logger.warning(f"âš ï¸ HTMLå†…å®¹å¼‚å¸¸ï¼Œé•¿åº¦: {len(html)}")
            return notes

        logger.info(f"ğŸ” å¼€å§‹ä»HTMLæå–é“¾æ¥ï¼ŒHTMLé•¿åº¦: {len(html)}")

        # ä»markdownè¯·æ±‚çš„æˆåŠŸç»“æœçœ‹ï¼Œé“¾æ¥æ ¼å¼æ˜¯è¿™æ ·çš„ï¼š
        # [](https://www.xiaohongshu.com/explore/682d7a17000000000f0338e2?xsec_token=...&xsec_source=)

        # æ›´ç²¾ç¡®çš„é“¾æ¥æ¨¡å¼ - åŸºäºå®é™…è§‚å¯Ÿåˆ°çš„æ ¼å¼
        patterns = [
            # åŒ¹é…markdownæ ¼å¼çš„é“¾æ¥ [](url)
            r'\]\((https://www\.xiaohongshu\.com/explore/[a-f0-9]{24}\?[^)]*)\)',
            # åŒ¹é…hrefå±æ€§ä¸­çš„é“¾æ¥
            r'href="(https://www\.xiaohongshu\.com/explore/[a-f0-9]{24}\?[^"]*)"',
            r'href="(/explore/[a-f0-9]{24}\?[^"]*)"',
            # åŒ¹é…å…¶ä»–å¯èƒ½çš„æ ¼å¼
            r'"(https://www\.xiaohongshu\.com/explore/[a-f0-9]{24}\?[^"]*)"',
            r'"(/explore/[a-f0-9]{24}\?[^"]*)"',
        ]

        all_matches = set()

        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, html, re.IGNORECASE)
            logger.info(f"ğŸ” æ¨¡å¼ {i+1} åŒ¹é…åˆ° {len(matches)} ä¸ªé“¾æ¥")

            for match in matches:
                # ç¡®ä¿æ˜¯å®Œæ•´çš„URL
                if match.startswith('/'):
                    full_url = urljoin(base_url, match)
                else:
                    full_url = match
                all_matches.add(full_url)

        logger.info(f"ğŸ” æ€»å…±å»é‡åæœ‰ {len(all_matches)} ä¸ªå”¯ä¸€é“¾æ¥")

        # å¦‚æœä¸»è¦æ¨¡å¼æ²¡æ‰¾åˆ°é“¾æ¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ
        if len(all_matches) == 0:
            logger.warning("âš ï¸ ä¸»è¦æ¨¡å¼æœªæ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ...")
            fallback_notes = self._fallback_extract_links(
                html, base_url, max_links)
            return fallback_notes

        # å¤„ç†æ‰¾åˆ°çš„é“¾æ¥
        for i, full_url in enumerate(list(all_matches)[:max_links]):
            note_id = self.parse_content_id_from_url(full_url)

            if note_id and len(note_id) >= 20:  # å°çº¢ä¹¦IDé€šå¸¸æ˜¯24ä½ï¼Œä½†è‡³å°‘è¦20ä½
                parsed = urlparse(full_url)
                query_params = parse_qs(parsed.query)

                has_xsec_token = "xsec_token" in query_params
                has_xsec_source = "xsec_source" in query_params

                note_info = {
                    "content_id": note_id,
                    "note_id": note_id,
                    "url": full_url,
                    "has_valid_token": has_xsec_token and has_xsec_source,
                    "tokens": list(query_params.keys()),
                    "preview_title": f"å°çº¢ä¹¦ç¬”è®° {note_id}",
                    "xsec_token": query_params.get("xsec_token", [None])[0],
                    "xsec_source": query_params.get("xsec_source", [None])[0],
                    "complete_params": has_xsec_token and has_xsec_source
                }

                notes.append(note_info)
                logger.info(
                    f"ğŸ” æ·»åŠ note: {note_id}, å®Œæ•´å‚æ•°: {note_info['complete_params']}")

        # ä¼˜å…ˆè¿”å›å‚æ•°å®Œæ•´çš„é“¾æ¥
        notes.sort(key=lambda x: x["complete_params"], reverse=True)
        logger.info(f"ğŸ” æœ€ç»ˆæå–å¹¶æ’åºå: {len(notes)} ä¸ªnotes")

        return notes

    def _parse_xiaohongshu_note(
        self,
        crawl_result,
        note_id: str,
        url: str
    ) -> Dict[str, Any]:
        """è§£æå°çº¢ä¹¦ç¬”è®°å†…å®¹ - ä¸“é—¨ä¼˜åŒ–"""

        html = getattr(crawl_result, 'html', '')
        markdown = getattr(crawl_result, 'markdown', '')

        # æå–æ ‡é¢˜
        title = self._extract_xiaohongshu_title(html, markdown)

        # æå–å†…å®¹
        content = self._clean_xiaohongshu_content(markdown)

        # æå–ä½œè€…
        author = self._extract_xiaohongshu_author(html)

        # æå–äº’åŠ¨æ•°æ®
        interaction_data = self._extract_xiaohongshu_interactions(html)

        # å¤„ç†åª’ä½“
        media_info = self._process_xiaohongshu_media(crawl_result)

        # æ£€æµ‹å†…å®¹ç±»å‹
        content_type = self._detect_xiaohongshu_content_type(html, media_info)

        return {
            "platform": self.config.name,
            "platform_display_name": self.config.display_name,
            "content_id": note_id,  # ğŸ†• æ·»åŠ è¿™è¡Œ
            "note_id": note_id,
            "url": url,
            "title": title,
            "content": content,
            "content_type": content_type,
            "author": author,
            "status_code": getattr(crawl_result, 'status_code', None),
            "crawled_at": datetime.now().isoformat(),
            "media_info": media_info,
            "raw_data": {
                "html_length": len(html),
                "markdown_length": len(markdown),
                "status_code": getattr(crawl_result, 'status_code', None)
            }
        }

    async def _get_xiaohongshu_browser_config(self) -> BrowserConfig:
        """è·å–ä¸“é—¨é’ˆå¯¹å°çº¢ä¹¦ä¼˜åŒ–çš„æµè§ˆå™¨é…ç½®"""
        # å‡è®¾ self.auth_service._create_auth_browser_config è¿”å›ä¸€ä¸ª BrowserConfig å®ä¾‹
        # æˆ–è€…ä½ åœ¨è¿™é‡Œæ„é€ å®ƒ
        base_config = self.auth_service._create_auth_browser_config(
            site_name=self.config.site_name,
            js_enabled=True,
            headless=True  # æœåŠ¡å™¨ä¸Šé€šå¸¸ä¸ºTrueï¼Œæœ¬åœ°è°ƒè¯•å¯è®¾ä¸ºFalse
        )

        # å¢åŠ é¡µé¢åŠ è½½åçš„é¢å¤–ç­‰å¾…æ—¶é—´ï¼ˆä¾‹å¦‚ï¼š3000æ¯«ç§’ = 3ç§’ï¼‰
        # è¿™ä¸ªå€¼å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼Œå¦‚æœé¡µé¢JSè¾ƒé‡ï¼Œå¯ä»¥é€‚å½“å¢åŠ 
        base_config.page_load_delay_ms = 3000  # <--- æ·»åŠ æˆ–ä¿®æ”¹æ­¤è¡Œ

        return base_config

    def parse_content_id_from_url(self, url: str) -> Optional[str]:
        """ä»URLä¸­è§£æç¬”è®°ID"""
        match = re.search(r'/explore/([a-f0-9]+)', url)
        return match.group(1) if match else None

    def _extract_xiaohongshu_title(self, html: str, markdown: str) -> str:
        """æå–å°çº¢ä¹¦ç¬”è®°æ ‡é¢˜"""
        # å°è¯•ä»HTMLä¸­æå–titleæ ‡ç­¾
        title_match = re.search(
            r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
        if title_match:
            title = title_match.group(1).strip()
            # æ¸…ç†å°çº¢ä¹¦æ ‡é¢˜ä¸­çš„åç¼€
            title = re.sub(r'\s*-\s*å°çº¢ä¹¦.*$', '', title)
            if len(title) > 3:
                return title

        # ä»markdownä¸­æå–ç¬¬ä¸€è¡Œæœ‰æ„ä¹‰çš„å†…å®¹
        if markdown:
            lines = markdown.strip().split('\n')
            for line in lines:
                line = line.strip()
                if line and len(line) > 3 and not line.startswith('http'):
                    # æ¸…ç†markdownæ ¼å¼
                    clean_line = re.sub(r'[#*`\[\]()]', '', line).strip()
                    if len(clean_line) > 3:
                        return clean_line[:50]

        return "å°çº¢ä¹¦ç¬”è®°"

    def _clean_xiaohongshu_content(self, markdown: str) -> str:
        """æ¸…ç†å°çº¢ä¹¦å†…å®¹"""
        if not markdown:
            return ""

        # ç§»é™¤å¯¼èˆªé“¾æ¥å’Œæ— å…³å†…å®¹
        lines = markdown.split('\n')
        clean_lines = []

        skip_patterns = [
            r'^https?://',
            r'^\[.*\]\(.*\)$',
            r'^(ç™»å½•|æ³¨å†Œ|ä¸‹è½½|Sign|Login)',
            r'^å°çº¢ä¹¦',
            r'^Medium Logo'
        ]

        for line in lines:
            line = line.strip()
            if line and not any(re.match(pattern, line, re.IGNORECASE) for pattern in skip_patterns):
                clean_lines.append(line)

        return '\n'.join(clean_lines)

    def _extract_xiaohongshu_author(self, html: str) -> Optional[str]:
        """æå–å°çº¢ä¹¦ä½œè€…ä¿¡æ¯"""
        # å¤šç§ä½œè€…æå–æ¨¡å¼
        author_patterns = [
            r'"author"\s*:\s*"([^"]+)"',
            r'"nickname"\s*:\s*"([^"]+)"',
            r'data-author="([^"]+)"',
            r'class="[^"]*author[^"]*"[^>]*>([^<]+)</[^>]*>',
        ]

        for pattern in author_patterns:
            match = re.search(pattern, html, re.IGNORECASE)
            if match:
                author = match.group(1).strip()
                if len(author) > 0 and len(author) < 50:
                    return author

        return None

    def _extract_xiaohongshu_interactions(self, html: str) -> Dict[str, Any]:
        """æå–å°çº¢ä¹¦äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€è¯„è®ºã€æ”¶è—ï¼‰"""
        interactions = {
            "likes_count": None,
            "comments_count": None,
            "collects_count": None,
            "shares_count": None
        }

        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æå–é€»è¾‘
        # ç”±äºå°çº¢ä¹¦é¡µé¢ç»“æ„å¤æ‚ï¼Œéœ€è¦æ ¹æ®å®é™…HTMLç»“æ„è°ƒæ•´

        return interactions

    def _process_xiaohongshu_media(self, crawl_result) -> Dict[str, Any]:
        """å¤„ç†å°çº¢ä¹¦åª’ä½“ä¿¡æ¯"""
        media_info = {
            "images": [],
            "videos": [],
            "total_count": 0
        }

        if hasattr(crawl_result, 'media') and crawl_result.media:
            if "images" in crawl_result.media:
                for img in crawl_result.media["images"]:
                    if isinstance(img, dict) and "src" in img:
                        # è¿‡æ»¤å°çº¢ä¹¦çš„æœ‰æ•ˆå›¾ç‰‡
                        src = img["src"]
                        if "xiaohongshu" in src or "xhscdn" in src:
                            media_info["images"].append({
                                "url": src,
                                "alt": img.get("alt", ""),
                                "width": img.get("width"),
                                "height": img.get("height")
                            })

        media_info["total_count"] = len(
            media_info["images"]) + len(media_info["videos"])
        return media_info

    def _detect_xiaohongshu_content_type(self, html: str, media_info: Dict) -> str:
        """æ£€æµ‹å°çº¢ä¹¦å†…å®¹ç±»å‹"""
        if media_info["total_count"] > 0:
            if len(media_info["videos"]) > 0:
                return "è§†é¢‘"
            elif len(media_info["images"]) > 0:
                return "å›¾æ–‡"

        # ä»HTMLä¸­æ£€æµ‹
        if "video" in html.lower() or "è§†é¢‘" in html:
            return "è§†é¢‘"
        elif "image" in html.lower() or "å›¾ç‰‡" in html:
            return "å›¾æ–‡"

        return "æ–‡å­—"

    # Tokenç¼“å­˜ç›¸å…³æ–¹æ³•
    def _get_cached_token(self) -> Optional[Dict[str, str]]:
        """è·å–ç¼“å­˜çš„token"""
        cache_time = self._token_cache.get("timestamp", 0)
        if datetime.now().timestamp() - cache_time < self._cache_ttl:
            return self._token_cache.get("token_info")
        return None

    def _cache_token(self, token_info: Dict[str, str]):
        """ç¼“å­˜tokenä¿¡æ¯"""
        self._token_cache = {
            "token_info": token_info,
            "timestamp": datetime.now().timestamp()
        }

    def _cache_token_from_url(self, url: str):
        """ä»URLä¸­æå–å¹¶ç¼“å­˜token"""
        token_info = self._extract_token_from_url(url)
        if token_info:
            self._cache_token(token_info)

    def _extract_token_from_url(self, url: str) -> Optional[Dict[str, str]]:
        """ä»URLä¸­æå–tokenä¿¡æ¯ - ä¿®å¤ç‰ˆæœ¬"""
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)

        token_info = {}

        # ç¡®ä¿æå–æ‰€æœ‰å¿…è¦å‚æ•°
        if "xsec_token" in query_params:
            token_info["xsec_token"] = query_params["xsec_token"][0]

        if "xsec_source" in query_params:
            token_info["xsec_source"] = query_params["xsec_source"][0]
        else:
            # ğŸ”§ å¦‚æœæ²¡æœ‰xsec_sourceï¼Œä½¿ç”¨é»˜è®¤å€¼
            token_info["xsec_source"] = "pc_feed"

        # ğŸ†• æ·»åŠ å…¶ä»–å¯èƒ½çš„å‚æ•°
        if "channel_id" in query_params:
            token_info["channel_id"] = query_params["channel_id"][0]

        return token_info if "xsec_token" in token_info else None

    def _build_note_url_with_token(self, note_id: str, token_info: Dict[str, str]) -> str:
        """ä½¿ç”¨tokenä¿¡æ¯æ„é€ ç¬”è®°URL - ä¿®å¤ç‰ˆæœ¬"""
        base_url = f"https://www.xiaohongshu.com/explore/{note_id}"

        params = {}

        # å¿…é¡»çš„å‚æ•°
        if "xsec_token" in token_info:
            params["xsec_token"] = token_info["xsec_token"]

        # ğŸ”§ ç¡®ä¿åŒ…å«xsec_sourceå‚æ•°
        if "xsec_source" in token_info:
            params["xsec_source"] = token_info["xsec_source"]
        else:
            params["xsec_source"] = "pc_feed"  # é»˜è®¤å€¼

        # å¯é€‰å‚æ•°
        if "channel_id" in token_info:
            params["channel_id"] = token_info["channel_id"]

        if params:
            param_string = urlencode(params)
            constructed_url = f"{base_url}?{param_string}"
            logger.info(f"ğŸ”— æ„é€ çš„å®Œæ•´URL: {constructed_url}")
            return constructed_url

        return base_url

    def _validate_note_url(self, url: str) -> bool:
        """éªŒè¯ç¬”è®°URLæ˜¯å¦åŒ…å«å¿…è¦å‚æ•°"""
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)

        has_xsec_token = "xsec_token" in query_params
        has_xsec_source = "xsec_source" in query_params

        if not has_xsec_token:
            logger.warning("âš ï¸ URLç¼ºå°‘xsec_tokenå‚æ•°")
        if not has_xsec_source:
            logger.warning("âš ï¸ URLç¼ºå°‘xsec_sourceå‚æ•°")

        return has_xsec_token and has_xsec_source

    def _fallback_extract_links(self, html: str, base_url: str, max_links: int) -> List[Dict[str, Any]]:
        """å¤‡é€‰çš„é“¾æ¥æå–æ–¹æ³• - åŸºäºå®é™…è§‚å¯Ÿçš„å†…å®¹"""
        notes = []

        # åŸºäºè§‚å¯Ÿåˆ°çš„å®é™…å†…å®¹ï¼Œåœ¨markdownä¸­å¯»æ‰¾é“¾æ¥
        logger.info("ğŸ”„ ä½¿ç”¨å¤‡é€‰æå–æ–¹æ³•...")

        # ä»å®é™…è¿”å›çš„markdownå†…å®¹ä¸­æå–
        # æ ¼å¼ï¼š[](https://www.xiaohongshu.com/explore/682d7a17000000000f0338e2?xsec_token=ABnu0gBGLZLlSE4VqHjDzM0xGRSAMvL7MlHaKrelWGeu8=&xsec_source=)

        # æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼
        simple_patterns = [
            r'explore/([a-f0-9]{20,})',  # ç›´æ¥æå–note_id
            r'/explore/([a-f0-9]+)\?',   # ä»URLè·¯å¾„ä¸­æå–
            r'xiaohongshu\.com/explore/([a-f0-9]+)',  # å®Œæ•´åŸŸååŒ¹é…
        ]

        found_ids = set()

        for pattern in simple_patterns:
            matches = re.findall(pattern, html, re.IGNORECASE)
            logger.info(f"ğŸ”„ å¤‡é€‰æ¨¡å¼æ‰¾åˆ° {len(matches)} ä¸ªID")

            for note_id in matches:
                if note_id and len(note_id) >= 20 and note_id not in found_ids:
                    found_ids.add(note_id)

                    # æ„é€ åŸºæœ¬URL
                    note_url = f"https://www.xiaohongshu.com/explore/{note_id}"

                    note_info = {
                        "content_id": note_id,
                        "note_id": note_id,
                        "url": note_url,
                        "has_valid_token": False,
                        "tokens": [],
                        "preview_title": f"å°çº¢ä¹¦ç¬”è®° {note_id}",
                        "xsec_token": None,
                        "xsec_source": None,
                        "complete_params": False
                    }

                    notes.append(note_info)
                    logger.info(f"ğŸ”„ å¤‡é€‰æ·»åŠ : {note_id}")

                    if len(notes) >= max_links:
                        break

        logger.info(f"ğŸ”„ å¤‡é€‰æ–¹æ³•æå–åˆ° {len(notes)} ä¸ªåŸºç¡€é“¾æ¥")
        return notes[:max_links]

    async def extract_content_links(
        self,
        source_url: Optional[str] = None,
        max_links: int = 20,
        **kwargs
    ) -> Dict[str, Any]:
        """æå–å°çº¢ä¹¦ç¬”è®°é“¾æ¥ - å¢å¼ºè°ƒè¯•ç‰ˆæœ¬"""
        try:
            if not source_url:
                source_url = self.config.default_source_url

            logger.info(f"ğŸ” æ­£åœ¨ä»å°çº¢ä¹¦æå–é“¾æ¥: {source_url}")

            browser_config = await self._get_xiaohongshu_browser_config()
            config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                page_timeout=60000,
                wait_for_images=True,
            )

            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=source_url, config=config)

                if not result.success:
                    raise CrawlerException(
                        message=f"è®¿é—®å°çº¢ä¹¦æ¢ç´¢é¡µå¤±è´¥: {result.error_message}",
                        error_type="access_failed"
                    )

                # ğŸ” å¢å¼ºHTMLå†…å®¹è°ƒè¯•
                html_content = getattr(result, 'html', '')
                markdown_content = getattr(result, 'markdown', '')

                logger.info(f"ğŸ” è·å–åˆ°çš„HTMLé•¿åº¦: {len(html_content)}")
                logger.info(f"ğŸ” è·å–åˆ°çš„Markdowné•¿åº¦: {len(markdown_content)}")

                # æ£€æŸ¥æ˜¯å¦åŒ…å«å°çº¢ä¹¦ç¬”è®°é“¾æ¥çš„ç‰¹å¾
                explore_count = html_content.count('/explore/')
                markdown_explore_count = markdown_content.count(
                    '/explore/') if markdown_content else 0

                logger.info(f"ğŸ” HTMLä¸­åŒ…å« {explore_count} ä¸ª /explore/ é“¾æ¥")
                logger.info(
                    f"ğŸ” Markdownä¸­åŒ…å« {markdown_explore_count} ä¸ª /explore/ é“¾æ¥")

                # ğŸ†• ä¼˜å…ˆä½¿ç”¨markdownå†…å®¹è¿›è¡Œæå–ï¼Œå› ä¸ºå®ƒæ›´å¹²å‡€
                content_to_parse = markdown_content if markdown_content and markdown_explore_count > 0 else html_content

                logger.info(
                    f"ğŸ” ä½¿ç”¨{'Markdown' if content_to_parse == markdown_content else 'HTML'}å†…å®¹è¿›è¡Œè§£æ")

                # ä½¿ç”¨ä¿®å¤åçš„é“¾æ¥æå–é€»è¾‘
                notes = self._extract_xiaohongshu_notes_from_html(
                    content_to_parse, source_url, max_links
                )

                # ğŸ” è¯¦ç»†çš„notesè°ƒè¯•ä¿¡æ¯
                logger.info(f"ğŸ” æå–åˆ°çš„notesæ•°é‡: {len(notes)}")
                if notes:
                    logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªnoteç¤ºä¾‹: {notes[0]}")
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰æå–åˆ°ä»»ä½•notes!")

                # æ„é€ raw_links
                raw_links = []
                for note in notes:
                    try:
                        raw_links.append({
                            "url": note.get("url", ""),
                            "note_id": note.get("note_id", ""),
                            "has_token": note.get("has_valid_token", False),
                            "query_params": note.get("tokens", [])
                        })
                    except Exception as e:
                        logger.warning(f"âš ï¸ å¤„ç†noteæ—¶å‡ºé”™: {str(e)}")
                        continue

                logger.info(f"âœ… æˆåŠŸæå– {len(notes)} ä¸ªå°çº¢ä¹¦ç¬”è®°é“¾æ¥")
                logger.info(f"ğŸ” raw_linksæ•°é‡: {len(raw_links)}")

                # ğŸ†• ç¡®ä¿è¿”å›å®Œæ•´çš„æ•°æ®ç»“æ„ï¼Œå³ä½¿notesä¸ºç©ºä¹Ÿè¦åŒ…å«å­—æ®µ
                result_data = {
                    "platform": self.config.name,
                    "platform_display_name": self.config.display_name,
                    "source_url": source_url,
                    "notes": notes,  # ç¡®ä¿åŒ…å«noteså­—æ®µ
                    "total_count": len(notes),
                    "extracted_at": datetime.now().isoformat(),
                    "raw_links": raw_links  # ç¡®ä¿åŒ…å«raw_linkså­—æ®µ
                }

                # ğŸ” æœ€ç»ˆæ•°æ®éªŒè¯
                logger.info(f"ğŸ” è¿”å›æ•°æ®keys: {list(result_data.keys())}")
                logger.info(
                    f"ğŸ” noteså­—æ®µç±»å‹: {type(result_data['notes'])}, é•¿åº¦: {len(result_data['notes'])}")
                logger.info(
                    f"ğŸ” raw_linkså­—æ®µç±»å‹: {type(result_data['raw_links'])}, é•¿åº¦: {len(result_data['raw_links'])}")

                return result_data

        except Exception as e:
            logger.error(f"âŒ å°çº¢ä¹¦é“¾æ¥æå–å¤±è´¥: {str(e)}")
            raise CrawlerException(
                message=f"å°çº¢ä¹¦é“¾æ¥æå–å¤±è´¥: {str(e)}",
                error_type="extract_failed"
            )
